{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- \\<Primer integrante\\>\n",
    "- \\<Segundo integrante\\>\n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: `\\<http://....\\>`\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import numba as nb\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import shutil\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiler():\n",
    "#TODO documentar los metodos\n",
    "    \"\"\" la clase profiler recibira un dataframe de pandas en el constructor\n",
    "    La funci√≥n summarize recibe una LISTA de strings de los nombres de las variables (nombre de las columnas del pandas) a las cuales se desea verle su\n",
    "    informacion respectiva \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,df:pd.DataFrame) -> None:\n",
    "        self.df = df\n",
    "        self.current_date = datetime.datetime.now().strftime(f\"%d-%m-%Y\")\n",
    "        self.EDA_directory_name = f\"EDA_{self.current_date}\"\n",
    "        self.PLOT_directory_name = f\"{self.EDA_directory_name}/plots\"\n",
    "        # Define the directory name\n",
    "        try:\n",
    "            if os.path.exists(self.EDA_directory_name):\n",
    "                shutil.rmtree(self.EDA_directory_name)\n",
    "            os.mkdir(self.EDA_directory_name)\n",
    "            print(f\"Directory '{self.EDA_directory_name}' created (overwritten) successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "    def summarize(self,column_name:str or list):\n",
    "        file_path = f\"{self.EDA_directory_name}/summary.txt\"\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write('\\n')\n",
    "\n",
    "        #TODO mejorar el reporte q va a summarize.txt\n",
    "        def report(_column_name):\n",
    "            \n",
    "            var =  self.df[_column_name]\n",
    "            report_dict = {\"variable name\": _column_name,\n",
    "                            \"dtype\": type(var[0]),\n",
    "                           \"unique values\": var.unique().shape[0],\n",
    "                           \"nulls\": var.isnull().sum()}\n",
    "            #print(f\"El tipo de variable para '{_column_name}' es {type(var[0])}.\")\n",
    "            #print(f\"Hay {var.unique().shape[0]} valores √∫nicos, correspondientes al {round(100*(var.unique().shape[0]/var.size),2)}% de la data.\")\n",
    "            #print(f\"Hay {var.isnull().sum()} valores nulos, correspondientes al {round(100*(var.isnull().sum()/var.size),2)}% de la data.\")\n",
    "\n",
    "            # Define the file path where you want to save the variables\n",
    "            \n",
    "            if var.dtype == int:\n",
    "                def detect_outliers(column):\n",
    "                    Q1 = column.quantile(0.25)\n",
    "                    Q3 = column.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    return (column < lower_bound) | (column > upper_bound)\n",
    "            \n",
    "                #Count rows with zero value\n",
    "                zero_count = (var == 0).sum()\n",
    "                # Count rows with negative values\n",
    "                negative_count = (var < 0).sum()\n",
    "                #Detect and count rows with outliers\n",
    "                outliers_count = detect_outliers(var).sum()\n",
    "                report_dict.update(var.describe().round(2).to_dict())\n",
    "                report_dict.update({\n",
    "                                \"zero count\": zero_count,\n",
    "                                \"negative count\": negative_count,\n",
    "                                \"outliers count\": outliers_count\n",
    "                            })\n",
    "                \n",
    "                # Use the describe method to calculate basic statistics\n",
    "                \n",
    "            with open(file_path, \"a\") as file:\n",
    "                for name, value in report_dict.items():\n",
    "                    file.write(\"\\n\")  # Add a newline for separation\n",
    "                    file.write(f\"{name}: {value}\")\n",
    "                file.write(\"\\n\")  # Add a newline for separation\n",
    "\n",
    "            print(f\"Variables have been exported to {file_path}\")\n",
    "\n",
    "        #try:\n",
    "        if isinstance(column_name, str):\n",
    "                report(column_name)        \n",
    "        elif isinstance(column_name, list):\n",
    "            for col in column_name:\n",
    "                report(col)    \n",
    "\n",
    "\n",
    "        #except Exception as e:\n",
    "         #   print(f'Exception Error: {e}')\n",
    "    #TODO change column_name for column_nameS\n",
    "    def plot_vars(self,column_name,N_adj = 10):\n",
    "        try:\n",
    "            if os.path.exists(self.PLOT_directory_name):\n",
    "                shutil.rmtree(self.PLOT_directory_name)\n",
    "            os.mkdir(self.PLOT_directory_name)\n",
    "            print(f\"Directory '{self.PLOT_directory_name}' created (overwritten) successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        def correlation_plot(variables:list):\n",
    "               #TODO revisar que los elementos de la lista sean todos str try/except\n",
    "            correlation_matrix = np.corrcoef(self.df[[variables]], rowvar=False)\n",
    "            plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "\n",
    "            # Create a heatmap using Seaborn\n",
    "            sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt='.2f', xticklabels=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5'], yticklabels=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5'])\n",
    "            plt.title('Correlation Matrix')\n",
    "\n",
    "            plt.show()\n",
    "        def histogram(var: pd.Series, N : int = N_adj):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "\n",
    "            # Calculate the histogram\n",
    "            hist_data, bin_edges, _ = plt.hist(var, bins=20)\n",
    "\n",
    "            # Sort the bins by frequency and select the top N\n",
    "            top_indices = np.argsort(hist_data)[::-1][:N]\n",
    "            top_bins = hist_data[top_indices]\n",
    "            top_edges = bin_edges[top_indices]\n",
    "\n",
    "            # Plot the histogram with the top N frequencies\n",
    "            #TODO quiza reemplazar por plotly\n",
    "            \n",
    "            #corregir este grafico            \n",
    "            plt.bar(top_edges, top_bins, width=np.diff(top_edges), color='blue', align='edge')\n",
    "            plt.title(f'Top {N} Frequencies Histogram')\n",
    "\n",
    "            plt.xlabel('Values')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "        def cramer_plot():\n",
    "            data = pd.crosstab(index=[\"Category1\", \"Category2\", \"Category3\"],\n",
    "                   columns=[\"FeatureA\", \"FeatureB\", \"FeatureC\"])\n",
    "\n",
    "            # Calculate Cramer's V\n",
    "            def cramers_v(confusion_matrix):\n",
    "                chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "                n = confusion_matrix.sum().sum()\n",
    "                phi2 = chi2 / n\n",
    "                r, k = confusion_matrix.shape\n",
    "                phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "                rcorr = r - ((r - 1)**2) / (n - 1)\n",
    "                kcorr = k - ((k - 1)**2) / (n - 1)\n",
    "                #return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1))\n",
    "\n",
    "            cramer_v = cramers_v(data)\n",
    "\n",
    "            # Create a heatmap to visualize Cramer's V\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(data.corr(method=cramers_v), annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "            plt.title(f\"Cramer's V Heatmap (V = {cramer_v:.2f})\")\n",
    "            plt.show()\n",
    "            \n",
    "        try: \n",
    "            if isinstance(column_name, str):\n",
    "                var = self.df[column_name]\n",
    "                if var.dtype == int:\n",
    "                    #TODO agregar xaxis al  densiity plot\n",
    "                    var.plot.density(title = f\"Plot de densidad para la variable {var.name}\")\n",
    "                    plt.show()\n",
    "            elif isinstance(column_name,list):\n",
    "                only_num = [i for i in column_name if isinstance(i,int or float)]\n",
    "                only_cat = [i for i in column_name if isinstance(i,str)]\n",
    "                for col in column_name:\n",
    "                    var = self.df[col]\n",
    "                    if var.dtype == int:\n",
    "                        var.plot.density(title = f\"Plot de densidad para la variable {column_name}\")\n",
    "                \n",
    "                if len(only_num) > 1:\n",
    "                    correlation_plot(only_num)\n",
    "                for var in only_cat:\n",
    "                    histogram(self.df[var])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Exception Error: {e}')     \n",
    "\n",
    "            \n",
    "    def clean_data():\n",
    "        pass\n",
    " \n",
    "    def scale():\n",
    "        pass\n",
    "    def make_clusters():\n",
    "        pass\n",
    "    def detect_anomalies():\n",
    "        pass\n",
    "    def profile():\n",
    "        pass\n",
    "    def clearGarbage():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Team</th>\n",
       "      <th>NOC</th>\n",
       "      <th>Games</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>City</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Event</th>\n",
       "      <th>Medal</th>\n",
       "      <th>age-height-weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A Dijiang</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1992 Summer</td>\n",
       "      <td>1992</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Basketball</td>\n",
       "      <td>Basketball Men's Basketball</td>\n",
       "      <td>None</td>\n",
       "      <td>24.0*180.0?80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Lamusi</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2012 Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Summer</td>\n",
       "      <td>London</td>\n",
       "      <td>Judo</td>\n",
       "      <td>Judo Men's Extra-Lightweight</td>\n",
       "      <td>None</td>\n",
       "      <td>23.0(170.0?60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Gunnar Nielsen Aaby</td>\n",
       "      <td>M</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1920 Summer</td>\n",
       "      <td>1920</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Football</td>\n",
       "      <td>Football Men's Football</td>\n",
       "      <td>None</td>\n",
       "      <td>24.0(nan?nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Edgar Lindenau Aabye</td>\n",
       "      <td>M</td>\n",
       "      <td>Denmark/Sweden</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1900 Summer</td>\n",
       "      <td>1900</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Tug-Of-War</td>\n",
       "      <td>Tug-Of-War Men's Tug-Of-War</td>\n",
       "      <td>Gold</td>\n",
       "      <td>34.0:nan?nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Christine Jacoba Aaftink</td>\n",
       "      <td>F</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NED</td>\n",
       "      <td>1988 Winter</td>\n",
       "      <td>1988</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>Speed Skating</td>\n",
       "      <td>Speed Skating Women's 500 metres</td>\n",
       "      <td>None</td>\n",
       "      <td>21.0(185.0?82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                      Name Sex            Team  NOC        Games  Year  \\\n",
       "0   1                 A Dijiang   M           China  CHN  1992 Summer  1992   \n",
       "1   2                  A Lamusi   M           China  CHN  2012 Summer  2012   \n",
       "2   3       Gunnar Nielsen Aaby   M         Denmark  DEN  1920 Summer  1920   \n",
       "3   4      Edgar Lindenau Aabye   M  Denmark/Sweden  DEN  1900 Summer  1900   \n",
       "4   5  Christine Jacoba Aaftink   F     Netherlands  NED  1988 Winter  1988   \n",
       "\n",
       "   Season       City          Sport                             Event Medal  \\\n",
       "0  Summer  Barcelona     Basketball       Basketball Men's Basketball  None   \n",
       "1  Summer     London           Judo      Judo Men's Extra-Lightweight  None   \n",
       "2  Summer  Antwerpen       Football           Football Men's Football  None   \n",
       "3  Summer      Paris     Tug-Of-War       Tug-Of-War Men's Tug-Of-War  Gold   \n",
       "4  Winter    Calgary  Speed Skating  Speed Skating Women's 500 metres  None   \n",
       "\n",
       "  age-height-weight  \n",
       "0   24.0*180.0?80.0  \n",
       "1   23.0(170.0?60.0  \n",
       "2      24.0(nan?nan  \n",
       "3      34.0:nan?nan  \n",
       "4   21.0(185.0?82.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'EDA_25-10-2023' created (overwritten) successfully.\n",
      "El tipo de variable para 'Year' es <class 'numpy.int64'>.\n",
      "Hay 35 valores √∫nicos, correspondientes al 0.01% de la data.\n",
      "Hay 0 valores nulos, correspondientes al 0.0% de la data.\n",
      "Variables have been exported to EDA_25-10-2023/summary.txt\n",
      "El tipo de variable para 'Name' es <class 'str'>.\n",
      "Hay 134732 valores √∫nicos, correspondientes al 49.7% de la data.\n",
      "Hay 0 valores nulos, correspondientes al 0.0% de la data.\n",
      "Variables have been exported to EDA_25-10-2023/summary.txt\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_parquet('olimpiadas.parquet')\n",
    "display(test.head())\n",
    "test_class = Profiler(test)\n",
    "\n",
    "#test_class.summarize('Year')\n",
    "test_class.summarize(['Year', 'Name'])\n",
    "\n",
    "#test_class.plot_vars(['Year',\"Name\"],N_adj=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\ndef __init__(self,DF):\\n        self.dataframe = DF #el constructor solamente recibira un dataframe\\n        self.today = datetime.today().strftime(\\'%d-%m-%Y\\')\\n        dir = \"EDA_\"+str(self.today)\\n        if not os.path.exists(dir):\\n            os.makedirs(dir)\\n    def summarize(self,vars): #uno entrega una lista [\"variable\"]\\n        selected = self.dataframe[vars]\\n        tipos = \\'\\n\\'.join(map(lambda x, y: f\"{x}: {y}\", vars, selected.dtypes))\\n        print(\"------------------------------------------------------------\")\\n        print(\"Los tipos de las variables son:\")\\n        print(tipos)\\n        nan_list = self.dataframe.isna().sum().to_list()\\n        nan_values = \\'\\n\\'.join(map(lambda x, y: f\"{x}: {y}\", vars, nan_list))\\n        print(\"------------------------------------------------------------\")\\n        print(\"Cantidad de NaNs por variable:\")\\n        print(nan_values)\\n        unique_list = self.dataframe.nunique().to_list()\\n        unique_values = \\'\\n\\'.join(map(lambda x, y: f\"{x}: {y}\", vars, unique_list))\\n        print(\"------------------------------------------------------------\")\\n        print(\"Cantidad de valores √∫nicos por variable:\")\\n        print(unique_values)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"    \n",
    "def __init__(self,DF):\n",
    "        self.dataframe = DF #el constructor solamente recibira un dataframe\n",
    "        self.today = datetime.today().strftime('%d-%m-%Y')\n",
    "        dir = \"EDA_\"+str(self.today)\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "    def summarize(self,vars): #uno entrega una lista [\"variable\"]\n",
    "        selected = self.dataframe[vars]\n",
    "        tipos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, selected.dtypes))\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        print(\"Los tipos de las variables son:\")\n",
    "        print(tipos)\n",
    "        nan_list = self.dataframe.isna().sum().to_list()\n",
    "        nan_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, nan_list))\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        print(\"Cantidad de NaNs por variable:\")\n",
    "        print(nan_values)\n",
    "        unique_list = self.dataframe.nunique().to_list()\n",
    "        unique_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, unique_list))\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        print(\"Cantidad de valores √∫nicos por variable:\")\n",
    "        print(unique_values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
