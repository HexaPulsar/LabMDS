{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vTs1ax9y6xN"
      },
      "source": [
        "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE5KA3OEy6xQ"
      },
      "source": [
        "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
        "\n",
        "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesor: Ignacio Meza, Gabriel Iturra\n",
        "- Auxiliar: Sebasti√°n Tinoco\n",
        "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
        "\n",
        "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
        "\n",
        "### Equipo:\n",
        "\n",
        "- Simon Repolt\n",
        "- Magdalena de la Fuente\n",
        "\n",
        "\n",
        "### Link de repositorio de GitHub: `https://github.com/HexaPulsar/LabMDS`\n",
        "\n",
        "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLU9ydRby6xQ"
      },
      "source": [
        "----\n",
        "\n",
        "## Reglas\n",
        "\n",
        "- **Grupos de 2 personas.**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Estrictamente prohibida la copia.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNpMtVMZy6xR"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
        "</div>\n",
        "\n",
        "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
        "\n",
        "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
        "- Caracterizaci√≥n autom√°tica de los datos\n",
        "- La soluci√≥n debe ser compatible con cualquier dataset\n",
        "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMutfXyfy6xR"
      },
      "source": [
        "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
        "\n",
        "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
        "\n",
        "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
        "\n",
        "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Reportar el tipo de variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
        "    - Si la variables es num√©rica:\n",
        "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
        "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
        "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
        "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
        "\n",
        "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/plots`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Para las variables num√©ricas:\n",
        "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
        "        - Grafique la correlaci√≥n entre las variables\n",
        "    - Para las variables categ√≥ricas:\n",
        "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
        "        - Grafique el coeficiente V de Cramer entre las variables\n",
        "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
        "    \n",
        "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clean_data`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Drop de valores duplicados\n",
        "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
        "        - Drop de valores nulos\n",
        "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
        "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
        "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
        "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
        "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
        "    - Deber√≠an usar `FunctionTransformer`.\n",
        "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
        "\n",
        "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/scale`\n",
        "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
        "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
        "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
        "        - Asuma que no existen datos ordinales en su dataset\n",
        "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
        "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
        "\n",
        "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clusters`\n",
        "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
        "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
        "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering.\n",
        "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
        "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "\n",
        "    - Crear la carpeta `EDA_fecha/anomalies`\n",
        "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
        "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
        "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a.\n",
        "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
        "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
        "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
        "\n",
        "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
        "\n",
        "Algunas consideraciones generales:\n",
        "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset.\n",
        "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
        "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
        "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 639,
      "metadata": {
        "id": "Kh8A54h0y6xT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import shutil\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler,FunctionTransformer,OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "from sklearn.base import is_classifier\n",
        "from sklearn.base import is_outlier_detector\n",
        "import IPython\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 640,
      "metadata": {
        "id": "pWG_VNX4y6xV"
      },
      "outputs": [],
      "source": [
        "#pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 641,
      "metadata": {
        "id": "XrDhWdwky6xW"
      },
      "outputs": [],
      "source": [
        "class Profiler():\n",
        "#TODO documentar los metodos\n",
        "    \"\"\" la clase profiler recibira un dataframe de pandas en el constructor\n",
        "    La funci√≥n summarize recibe una LISTA de strings de los nombres de las variables (nombre de las columnas del pandas) a las cuales se desea verle su\n",
        "    informacion respectiva\n",
        "    \"\"\"\n",
        "    def __init__(self,df:pd.DataFrame) -> None:\n",
        "        self.df = df\n",
        "        self.current_date = datetime.datetime.now().strftime(f\"%d-%m-%Y\")\n",
        "\n",
        "        self.EDA_directory_name = f\"EDA_{self.current_date}\"\n",
        "        self.PLOT_directory_name = f\"{self.EDA_directory_name}/plots\"\n",
        "        self.CLEAN_directory_name = f\"{self.EDA_directory_name}/clean_data\"\n",
        "        self.CLUSTERS_directory_name = f\"{self.EDA_directory_name}/clusters\"\n",
        "        self.SCALE_directory_name = f\"{self.EDA_directory_name}/scaled\"\n",
        "\n",
        "        self.categorical = self.df.select_dtypes(include=['object'])  # or exclude=['number'] for older versions\n",
        "        self.numerical = self.df.select_dtypes(include=['number'])  # or include=['number'] for older versions\n",
        "\n",
        "        # Define the directory name\n",
        "        try:\n",
        "            if os.path.exists(self.EDA_directory_name):\n",
        "                shutil.rmtree(self.EDA_directory_name)\n",
        "            os.mkdir(self.EDA_directory_name)\n",
        "            print(f\"Directory '{self.EDA_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "    def summarize(self,column_name:str or list=None):\n",
        "        \"\"\"summarize: recibe  un string con el nombre de una columna o una lista que contenga el nombre de las columnas a realizar resumen\n",
        "        Si se desea realizar un resumen al dataframe completo dejar la variable everything como True y column_name como None.\n",
        "        En caso de que se desee realizar un resumen por variable(s) seleccionada(s), dejar everything en falso e indicar la o las variables\n",
        "        en el formato ya dicho.\n",
        "\n",
        "        Esta funci√≥n provee un txt guardado en la carpeta EDA_fecha con informacion del dataset o las variables, segun corresponda\n",
        "\n",
        "        Raises:\n",
        "            Exception: _description_\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        def detect_outliers(column):\n",
        "            Q1 = column.quantile(0.25)\n",
        "            Q3 = column.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = (column < lower_bound) | (column > upper_bound)\n",
        "            return outliers.sum()\n",
        "\n",
        "        file_path = f\"{self.EDA_directory_name}/summary.txt\"\n",
        "\n",
        "        def report_txt(vars):\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    #print(\"Writing types...\")\n",
        "                    tipos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.dtypes))\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    f.write(\"Los tipos de las variables son: \"+\"\\n\")\n",
        "                    f.write(tipos)\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    #print(\"Writing NaNs...\")\n",
        "                    nan_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.isna().sum().to_list()))\n",
        "                    f.write(\"Cantidad de NaNs por variable: \"+\"\\n\")\n",
        "                    f.write(nan_values)\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    #print(\"Writing unique values...\")\n",
        "                    unique_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.nunique().to_list()))\n",
        "                    f.write(\"Cantidad de valores √∫nicos por variable: \"+\"\\n\")\n",
        "                    f.write(unique_values)\n",
        "                    if self.numerical.shape[0] > 1: #si no hay numericas no hacer nada de esto\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        nceros = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, (self.numerical == 0).sum().to_list()))\n",
        "                        f.write(\"Cantidad de ceros en las variables numericas: \"+\"\\n\")\n",
        "                        f.write(nceros)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        promedios =  '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.mean().to_list()))\n",
        "                        f.write(\"Promedios de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(promedios)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        negativos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, (self.numerical < 0).sum().to_list()))\n",
        "                        f.write(\"Cantidad de negativos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(negativos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        maximos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.max().to_list()))\n",
        "                        f.write(\"Maximos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(maximos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        minimos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.min().to_list()))\n",
        "                        f.write(\"M√≠nimos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(minimos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        outliers = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.apply(detect_outliers)))\n",
        "                        f.write(\"Outliers de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(outliers)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        values_perc = [0.25, 0.5, 0.75, 1.0]\n",
        "                        percentiles = self.numerical.quantile(values_perc).values\n",
        "                        for idx,perc in enumerate(values_perc):\n",
        "                            f.write(\"Percentil \" + str(perc*100) + \" de las variables numericas: \"+\"\\n\")\n",
        "                            perc_val = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, percentiles[idx]))\n",
        "                            f.write(perc_val)\n",
        "                            f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    f.close()\n",
        "                    print(f\"Summary.txt has been successfully exported too {self.EDA_directory_name}\")\n",
        "            with open(file_path, 'r') as file:\n",
        "                # Read the entire content of the file\n",
        "                file_content = file.read()\n",
        "                # Print the content to the console\n",
        "                print(file_content)\n",
        "\n",
        "        try:\n",
        "            if column_name is None:\n",
        "              report_txt(self.df)\n",
        "\n",
        "            elif isinstance(column_name, str):\n",
        "                var = self.df[[column_name]]\n",
        "                report_txt(var)\n",
        "\n",
        "            elif isinstance(column_name,list):\n",
        "                if not column_name:\n",
        "                    raise ValueError(\"The input columns list is empty\")\n",
        "                vars =  self.df[column_name]\n",
        "                report_txt(vars)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Exception Error: {e}')\n",
        "\n",
        "\n",
        "    def plot_vars(self,column_name= None,N_adj = 10):\n",
        "        \"\"\"\n",
        "        Plots various visualizations for the specified column(s) in the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            column_name (str or list): The name of the column or a list of column names to visualize.\n",
        "            N_adj (int, optional): The number of top categories to consider in histograms. Defaults to 10.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the input columns list is empty.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.PLOT_directory_name):\n",
        "                shutil.rmtree(self.PLOT_directory_name)\n",
        "            os.mkdir(self.PLOT_directory_name)\n",
        "            print(f\"Directory '{self.PLOT_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        def correlation_plot(variables:list):\n",
        "\n",
        "            \"\"\"\n",
        "            Plots a correlation matrix heatmap for a list of numerical variables.\n",
        "\n",
        "            Args:\n",
        "                variables (list): A list of column names representing numerical variables.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "\n",
        "            #TODO revisar que los elementos de la lista sean todos str try/except\n",
        "            correlation_matrix = np.corrcoef(self.numerical[variables], rowvar=False)\n",
        "            plt.figure()  # Set the figure size\n",
        "\n",
        "            # Create a heatmap using Seaborn\n",
        "            sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt='.2f', xticklabels=variables, yticklabels=variables)\n",
        "            plt.title('Correlation Matrix')\n",
        "\n",
        "\n",
        "        def histogram(var: pd.Series, N: int):\n",
        "            \"\"\"\n",
        "            Plots a histogram for a categorical variable.\n",
        "\n",
        "            Args:\n",
        "                var (pd.Series): A pandas Series representing a categorical variable.\n",
        "                N (int): The number of top categories to display in the histogram.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "            category_counts = var.value_counts()\n",
        "            top_N_categories = category_counts.head(N)\n",
        "            plt.bar(top_N_categories.index, top_N_categories.values)\n",
        "            plt.xlabel('Category')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.title(f'Top {N} Categories Histogram for variable {var.name}')\n",
        "\n",
        "        def cramers_v(confusion_matrix):\n",
        "            \"\"\"\n",
        "            Calculates Cramer's V statistic for two categorical variables.\n",
        "\n",
        "            Args:\n",
        "                confusion_matrix (pd.DataFrame): The contingency table of two categorical variables.\n",
        "\n",
        "            Returns:\n",
        "                float: The Cramer's V statistic.\n",
        "\n",
        "            Notes:\n",
        "                Cramer's V is a measure of association for categorical variables.\n",
        "\n",
        "            \"\"\"\n",
        "            chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "            n = confusion_matrix.sum().sum()\n",
        "            phi2 = chi2 / n\n",
        "            r, k = confusion_matrix.shape\n",
        "            phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
        "            rcorr = r - ((r - 1)**2) / (n - 1)\n",
        "            kcorr = k - ((k - 1)**2) / (n - 1)\n",
        "            return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "        def cramer_v_plot(variables):\n",
        "            \"\"\"\n",
        "            Plots a heatmap of Cramer's V values for all pairs of categorical variables.\n",
        "\n",
        "            Args:\n",
        "                variables (list): A list of column names representing categorical variables.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "            # Extract categorical columns from the DataFrame\n",
        "            categorical_cols = self.categorical[variables]\n",
        "\n",
        "            # Calculate Cramer's V for all pairs of categorical columns\n",
        "            results = pd.DataFrame(index=categorical_cols.columns, columns=categorical_cols.columns, dtype=float)\n",
        "\n",
        "            for col1 in categorical_cols.columns:\n",
        "                for col2 in categorical_cols.columns:\n",
        "                    confusion_matrix = pd.crosstab(categorical_cols[col1], categorical_cols[col2])\n",
        "                    results.loc[col1, col2] = cramers_v(confusion_matrix)\n",
        "\n",
        "            # Create a heatmap to visualize Cramer's V\n",
        "            plt.figure()\n",
        "            sns.heatmap(results, annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
        "            plt.title(\"Cramer's V Heatmap\")\n",
        "\n",
        "        try:\n",
        "            if column_name is None:\n",
        "                present_num = self.numerical\n",
        "                present_cat = self.categorical\n",
        "\n",
        "                if len(present_num) > 1:\n",
        "                    correlation_plot(present_num.columns)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/corr.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                if len(present_cat) > 1:\n",
        "                    cramer_v_plot(present_cat.columns)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/cramer_v.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                for col in self.df.columns:\n",
        "                    if col in self.numerical.columns:\n",
        "                        var = self.df[col]\n",
        "                        var.plot.density(title = f\"Plot de densidad para la variable {col}\")\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "                    if col in self.categorical.columns:\n",
        "                    #TODO agregar N top categories en histplot\n",
        "                        histogram(self.df[col],N= N_adj)\n",
        "                        plt.gcf()  # Get the current figure\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "\n",
        "            if isinstance(column_name, str):\n",
        "                var = self.df[column_name]\n",
        "                var.plot.density(title=f\"Plot de densidad para la variable {column_name}\")\n",
        "                plt.savefig(f\"{self.PLOT_directory_name}/{column_name}.pdf\", format = 'pdf')\n",
        "                plt.show()\n",
        "\n",
        "            elif isinstance(column_name,list):\n",
        "                present_num = [i for i in self.numerical if i in column_name]\n",
        "                present_cat = [i for i in self.categorical if i in column_name]\n",
        "\n",
        "                if present_num and len(present_num) > 1:\n",
        "                    correlation_plot(present_num)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/corr.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                if present_cat and len(present_cat) > 1:\n",
        "                    cramer_v_plot(present_cat)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/cramer_v.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                for col in column_name:\n",
        "                    if col in self.numerical.columns:\n",
        "                        var = self.df[col]\n",
        "                        var.plot.density(title = f\"Plot de densidad para la variable {col}\")\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "                    if col in self.categorical.columns:\n",
        "                    #TODO agregar N top categories en histplot\n",
        "                        histogram(self.df[col],N= N_adj)\n",
        "                        plt.gcf()  # Get the current figure\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Exception Error: {e}')\n",
        "\n",
        "\n",
        "    def clean_data(self,non_atomic:list=None,column_name: str or list= None, everything=True, nan_treatment = \"delete\"):\n",
        "        \"\"\"\n",
        "        Cleans the data based on the specified parameters and generates a cleaned .csv file.\n",
        "\n",
        "        Args:\n",
        "            non_atomic (list or None): A list of non-atomic column names in the dataset.\n",
        "            If there is only one non-atomic column, it should still be provided as a list. If there are no non-atomic columns, set this parameter to None.\n",
        "            If you want to clean the atomic column along with specific other variables, include the atomic column name in column_name.\n",
        "            column_name (list or str): A list of column names or a single column name to be cleaned.\n",
        "            everything (bool): Set to True to clean the entire dataset. If True, non_atomic should be provided as needed, and column_name should be None.\n",
        "            nan_treatment (str): Options for handling NaN values, which include: \"delete\" (remove rows with NaNs), \"mean\" (replace NaNs with the column's mean for numerical columns), and \"zeros\" (replace NaNs with zeros).\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Notes:\n",
        "            This function generates a .csv file containing the cleaned variables in the 'EDA_fecha/clean/' folder.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.CLEAN_directory_name):\n",
        "                shutil.rmtree(self.CLEAN_directory_name)\n",
        "            os.mkdir(self.CLEAN_directory_name)\n",
        "            print(f\"Directory '{self.CLEAN_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        file_path=f\"{self.CLEAN_directory_name}/data.csv\"\n",
        "        if everything:\n",
        "            to_clean = self.df\n",
        "            columnas = to_clean.columns.to_list()\n",
        "        else:\n",
        "            to_clean =  self.df[column_name]\n",
        "            columnas = column_name\n",
        "        def atomize_column(df, column_name_not_atomic):\n",
        "            split_names = column_name_not_atomic.split('-')\n",
        "            n_splits = len(split_names)\n",
        "            new_df = pd.DataFrame(columns=split_names)\n",
        "\n",
        "            def process_split(x):\n",
        "                split = re.findall(r'\\d+\\.\\d+', x)\n",
        "                if len(split) < n_splits:\n",
        "                    split += [np.nan] * (n_splits - len(split))\n",
        "                return {split_names[i]: split[i] for i in range(n_splits)}\n",
        "\n",
        "            non_atomic_data = df[column_name_not_atomic].apply(process_split)\n",
        "            new_df = pd.DataFrame(list(non_atomic_data))\n",
        "            return new_df\n",
        "\n",
        "        def clean_data_func(df, non_atomic=None, column_name=None, everything=True, nan_treatment=\"delete\"):\n",
        "            for i in non_atomic:\n",
        "                to_add = atomize_column(df,i)\n",
        "                df=df.drop(i,axis=1)\n",
        "                df=pd.concat([df,to_add])\n",
        "            df.drop_duplicates()\n",
        "            if nan_treatment == \"delete\":\n",
        "                df=df.dropna()\n",
        "            elif nan_treatment == \"mean\":\n",
        "                df=df.fillna(df.mean())\n",
        "            elif nan_treatment == \"zeros\":\n",
        "                df=df.fillna(0)\n",
        "            else:\n",
        "                raise Exception(\"Tratamiento no disponible!\")\n",
        "            display(df)\n",
        "            return df\n",
        "\n",
        "        clean_data_transformer = FunctionTransformer(clean_data_func, \\\n",
        "            kw_args={'non_atomic': non_atomic, 'column_name': column_name, 'everything': everything, 'nan_treatment': nan_treatment})\n",
        "\n",
        "        # Apply the FunctionTransformer to your DataFrame\n",
        "        df_limpio = clean_data_transformer.transform(to_clean)\n",
        "\n",
        "        # Save the cleaned DataFrame to a CSV file\n",
        "        df_limpio.to_csv(f\"{self.EDA_directory_name}/clean_data/data.csv\", index=False)\n",
        "        print(\"porcentaje de data retenida: \",df_limpio.shape[0]/to_clean.shape[0])\n",
        "        df_limpio.to_csv(file_path)\n",
        "        #self.df = df_limpio\n",
        "\n",
        "\n",
        "    \"\"\"Scale: Este metodo escala los datos numericos primero aplicando logaritmo y despues un escalamiento MinMax\n",
        "\n",
        "    En caso de que se desee realizar escalamiento a los datos categ√≥ricos, ingresar un escalamiento del framework de sklearn\n",
        "    en la variable categorical_method. Si no, dejar como None\n",
        "\n",
        "    Este metodo guarda los datos escalados a partir de los datos limpios guardados en el metodo anterior. Los datos escalados\n",
        "    son guardados en la carpeta EDA_fecha/scale\n",
        "    \"\"\"\n",
        "    def scale(self,categorical_method=OneHotEncoder()):\n",
        "        \"\"\"\n",
        "        Scales the numerical data by first applying logarithm transformation and then using Min-Max scaling.\n",
        "\n",
        "        Args:\n",
        "        categorical_method: Scikit-learn scaling method to be applied to categorical data. If scaling is not needed for categorical data, leave this parameter as None.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "\n",
        "        Notes:\n",
        "        This method scales the data based on the previously cleaned data and saves the scaled data in the 'EDA_fecha/scale' folder.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.SCALE_directory_name):\n",
        "                shutil.rmtree(self.SCALE_directory_name)\n",
        "            os.mkdir(self.SCALE_directory_name)\n",
        "            print(f\"Directory '{self.SCALE_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        file_path = f\"{self.SCALE_directory_name}/scaled_features.csv\"\n",
        "        clean_path = f\"{self.CLEAN_directory_name}/data.csv\"\n",
        "        clean_data = pd.read_csv(clean_path)\n",
        "\n",
        "        categoricas = clean_data.select_dtypes(include=['object']).columns\n",
        "        numericas = clean_data.select_dtypes(include=['number']).columns\n",
        "\n",
        "        numericas_transf = Pipeline(steps=[('log', FunctionTransformer(func=np.log1p)),('minmax', MinMaxScaler())])\n",
        "        if categorical_method != None:\n",
        "            categoricas_transf = Pipeline(steps=[('categoricas', categorical_method)])\n",
        "            preprocessor = ColumnTransformer(transformers=[('var_cat', categoricas_transf, categoricas),('var_num', numericas_transf, numericas)])\n",
        "        else:\n",
        "            preprocessor = ColumnTransformer(transformers=[('var_num', numericas_transf, numericas)])\n",
        "        data_procesada = preprocessor.fit_transform(clean_data)\n",
        "        df_final= pd.DataFrame(data_procesada)\n",
        "        df_final.to_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def make_clusters(self, clustering_algorithm, n_clusters = None, columns_name=None):\n",
        "        \"\"\"Create clusters using K-Means and visualize the results.\n",
        "\n",
        "        This function creates clusters from the input data using K-Means clustering.\n",
        "        It can also automatically determine the optimal number of clusters using\n",
        "        the elbow method. The resulting clusters are visualized in a scatter plot\n",
        "        using PCA.\n",
        "\n",
        "        Args:\n",
        "            n_clusters (int, optional): The number of clusters to create. If None, the\n",
        "                function will automatically determine the optimal number of clusters\n",
        "                using the elbow method. Defaults to None.\n",
        "            columns_name (list, optional): List of column names to use for clustering.\n",
        "                If None, all columns from the DataFrame are used. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Note:\n",
        "            If `n_clusters` is not provided, this function will determine the optimal\n",
        "            number of clusters using the elbow method. It is recommended to set\n",
        "            `n_clusters` to None for optimal results.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.CLUSTERS_directory_name):\n",
        "                shutil.rmtree(self.CLUSTERS_directory_name)\n",
        "            os.mkdir(self.CLUSTERS_directory_name)\n",
        "            print(f\"Directory '{self.CLUSTERS_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        if columns_name == None:\n",
        "            copy_df = self.df\n",
        "        else:\n",
        "            copy_df = self.df[columns_name]\n",
        "            #TODO add preprocessing just in case\n",
        "\n",
        "\n",
        "        def auto_elbow_method(data,n_clusters_range:np.linspace = np.linspace(2,10, 8,dtype = int),_n_init = 5, plot_elbow = True,**kwargs):\n",
        "            \"\"\"auto elbow method for kmeans\n",
        "\n",
        "            Args:\n",
        "                data (_type_): data to clusterize\n",
        "                n_clusters_range (np.linspace): range of n_clusters for elbow method\n",
        "                _n_init (int, optional): number of kmeans init. Defaults to 5.\n",
        "                _random_state (int, optional): for replicability. Defaults to 42.\n",
        "                plot (bool, optional): plots elbow method. Defaults to False.\n",
        "\n",
        "            Returns:\n",
        "                _n_clusters_ (int): returns number of clusters for kmeans\n",
        "            \"\"\"\n",
        "            # Range of cluster numbers to try\n",
        "            # Initialize an empty list to store the variance explained by each cluster\n",
        "            inertia = []\n",
        "\n",
        "            # Perform K-Means clustering for different values of k\n",
        "            for n_clusters in tqdm(n_clusters_range,desc='testing clusters in elbow method'):\n",
        "                kmeans = KMeans(n_clusters=n_clusters,n_init=_n_init)\n",
        "                kmeans.fit(data)\n",
        "                inertia.append(kmeans.inertia_)\n",
        "\n",
        "            if plot_elbow:\n",
        "                # Create the Elbow Method graph\n",
        "                plt.figure()\n",
        "                plt.plot(n_clusters_range, inertia, marker='o')\n",
        "                plt.title('Elbow Method for Optimal K')\n",
        "                plt.xlabel('Number of Clusters (K)')\n",
        "                plt.ylabel('Variance Explained (Inertia)')\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "            variation = [(inertia[i] - inertia[i+1])/ inertia[i] * 100 for i in range(len(inertia)-1)]\n",
        "            n_clusters = n_clusters_range[variation.index(max(variation)) + 1]\n",
        "            print(f\"Optimal n_clusters is {int(n_clusters)}\")\n",
        "            return n_clusters\n",
        "\n",
        "        pca = PCA(n_components=2)\n",
        "        if isinstance(clustering_algorithm,KMeans) and n_clusters is None:\n",
        "            print(f'n_clusters is set to None. n_clusters will be initialized via the internal auto_elbow_method function.')\n",
        "            n_clusters =  auto_elbow_method(self.df)\n",
        "            clustering_algorithm.set_params(n_clusters = n_clusters)\n",
        "        # Create a pipeline with PCA and KMeans\n",
        "        cluster_pipeline = Pipeline([\n",
        "        ('clustering', clustering_algorithm),\n",
        "         ('pca', pca)\n",
        "        ])\n",
        "        cluster_labels = cluster_pipeline['clustering'].fit_predict(self.df)\n",
        "        X_pca = cluster_pipeline['pca'].fit_transform(self.df)\n",
        "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
        "        plt.title('PCA Scatter Plot with Clusters')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.show()\n",
        "\n",
        "    def detect_anomalies(self,algoritmo, algoritmo_params=None,tipo=\"limpio\"):\n",
        "        try:\n",
        "            if os.path.exists(self.ANOMALIE_directory_name):\n",
        "                shutil.rmtree(self.ANOMALIE_directory_name)\n",
        "            os.mkdir(self.ANOMALIE_directory_name)\n",
        "            print(f\"Directory '{self.CLUSTERS_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        path_file=f'{self.CLUSTERS_directory_name}/data_anomalies.csv'\n",
        "        scaled_path = f\"{self.SCALE_directory_name}/scaled_features.csv\"\n",
        "        clean_path = f\"{self.CLEAN_directory_name}/data.csv\"\n",
        "        if tipo == \"limpio\":\n",
        "            data = pd.read_csv(clean_path)\n",
        "        if tipo == \"escalada\":\n",
        "            data = pd.read_csv(scaled_path)\n",
        "        else:\n",
        "            data = self.df\n",
        "\n",
        "        if algoritmo_params is None:\n",
        "            algoritmo_params = {}\n",
        "\n",
        "        if not (is_classifier(algoritmo) or is_outlier_detector(algoritmo)):\n",
        "            raise ValueError(\"Algoritmo debe ser parte del framework de sklearn\")\n",
        "\n",
        "        model = algoritmo.set_params(**algoritmo_params)\n",
        "        pca = PCA(n_components=2)\n",
        "\n",
        "        pipeline = Pipeline([('pca', pca),('anomaly_detection', model)])\n",
        "\n",
        "        pipeline.fit(data)\n",
        "        predictions = pipeline.predict(data)\n",
        "        data['Anomalias'] = predictions\n",
        "\n",
        "        plt.scatter(data['PC1'], data['PC2'], c=data['Anomalias'], cmap=plt.cm.Paired, s=20)\n",
        "        plt.title('Anomaly Detection Results')\n",
        "        plt.xlabel('Componente 1')\n",
        "        plt.ylabel('Componente 2')\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "        data.to_csv(path_file)\n",
        "        plt.savefig(f\"{self.ANOMALIE_directory_name}/Anomaliasplot.pdf\", format = 'pdf')\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Este metodo ejecuta todos los metodos de la clase exceptuando clearGarbage y este mismo metodo\n",
        "    Cabe decir que se ejecutar√°n todos los metodos en su configuraci√≥n por defecto\n",
        "    \"\"\"\n",
        "    def profile(self):\n",
        "        \"\"\"\n",
        "        Executes all callable methods of the class, excluding specific methods.\n",
        "\n",
        "        Calls all callable methods of the class except those listed in 'no_incluir' list.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        metodos = [method for method in dir(self) if callable(getattr(self, method)) and not method.startswith(\"__\")]\n",
        "        no_incluir = [\"profile\", \"clearGarbage\"]\n",
        "        for nombre in metodos:\n",
        "            if nombre not in no_incluir:\n",
        "                metodo = getattr(self, nombre)\n",
        "                metodo()\n",
        "\n",
        "    \"Este metodo borra todos los contenidos de la carpeta EDA_fecha, pero no esta carpeta.\"\n",
        "    def clearGarbage(self):\n",
        "        \"\"\"\n",
        "        Deletes all contents within the 'EDA_fecha' folder but leaves the folder itself.\n",
        "\n",
        "        Removes all files and subfolders within the 'EDA_fecha' directory while keeping the directory itself.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        for raiz, carpetas, archivos in os.walk(self.EDA_directory_name, topdown=False):\n",
        "            for folder in carpetas:\n",
        "                folder_path = os.path.join(raiz, folder)\n",
        "                shutil.rmtree(folder_path)\n",
        "            for file in archivos:\n",
        "                file_path = os.path.join(raiz, file)\n",
        "                os.remove(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 642,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNEkogGrUPjn",
        "outputId": "2c06ef0a-272d-44fb-ebb5-932bc9fa8d5f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Team</th>\n",
              "      <th>NOC</th>\n",
              "      <th>Games</th>\n",
              "      <th>Year</th>\n",
              "      <th>Season</th>\n",
              "      <th>City</th>\n",
              "      <th>Sport</th>\n",
              "      <th>Event</th>\n",
              "      <th>Medal</th>\n",
              "      <th>age-height-weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A Dijiang</td>\n",
              "      <td>M</td>\n",
              "      <td>China</td>\n",
              "      <td>CHN</td>\n",
              "      <td>1992 Summer</td>\n",
              "      <td>1992</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Barcelona</td>\n",
              "      <td>Basketball</td>\n",
              "      <td>Basketball Men's Basketball</td>\n",
              "      <td>None</td>\n",
              "      <td>24.0*180.0?80.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A Lamusi</td>\n",
              "      <td>M</td>\n",
              "      <td>China</td>\n",
              "      <td>CHN</td>\n",
              "      <td>2012 Summer</td>\n",
              "      <td>2012</td>\n",
              "      <td>Summer</td>\n",
              "      <td>London</td>\n",
              "      <td>Judo</td>\n",
              "      <td>Judo Men's Extra-Lightweight</td>\n",
              "      <td>None</td>\n",
              "      <td>23.0(170.0?60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Gunnar Nielsen Aaby</td>\n",
              "      <td>M</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>DEN</td>\n",
              "      <td>1920 Summer</td>\n",
              "      <td>1920</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Antwerpen</td>\n",
              "      <td>Football</td>\n",
              "      <td>Football Men's Football</td>\n",
              "      <td>None</td>\n",
              "      <td>24.0(nan?nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Edgar Lindenau Aabye</td>\n",
              "      <td>M</td>\n",
              "      <td>Denmark/Sweden</td>\n",
              "      <td>DEN</td>\n",
              "      <td>1900 Summer</td>\n",
              "      <td>1900</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Paris</td>\n",
              "      <td>Tug-Of-War</td>\n",
              "      <td>Tug-Of-War Men's Tug-Of-War</td>\n",
              "      <td>Gold</td>\n",
              "      <td>34.0:nan?nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Christine Jacoba Aaftink</td>\n",
              "      <td>F</td>\n",
              "      <td>Netherlands</td>\n",
              "      <td>NED</td>\n",
              "      <td>1988 Winter</td>\n",
              "      <td>1988</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Calgary</td>\n",
              "      <td>Speed Skating</td>\n",
              "      <td>Speed Skating Women's 500 metres</td>\n",
              "      <td>None</td>\n",
              "      <td>21.0(185.0?82.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271111</th>\n",
              "      <td>135569</td>\n",
              "      <td>Andrzej ya</td>\n",
              "      <td>M</td>\n",
              "      <td>Poland-1</td>\n",
              "      <td>POL</td>\n",
              "      <td>1976 Winter</td>\n",
              "      <td>1976</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Innsbruck</td>\n",
              "      <td>Luge</td>\n",
              "      <td>Luge Mixed (Men)'s Doubles</td>\n",
              "      <td>None</td>\n",
              "      <td>29.0:179.0?89.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271112</th>\n",
              "      <td>135570</td>\n",
              "      <td>Piotr ya</td>\n",
              "      <td>M</td>\n",
              "      <td>Poland</td>\n",
              "      <td>POL</td>\n",
              "      <td>2014 Winter</td>\n",
              "      <td>2014</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Sochi</td>\n",
              "      <td>Ski Jumping</td>\n",
              "      <td>Ski Jumping Men's Large Hill, Individual</td>\n",
              "      <td>None</td>\n",
              "      <td>27.0:176.0?59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271113</th>\n",
              "      <td>135570</td>\n",
              "      <td>Piotr ya</td>\n",
              "      <td>M</td>\n",
              "      <td>Poland</td>\n",
              "      <td>POL</td>\n",
              "      <td>2014 Winter</td>\n",
              "      <td>2014</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Sochi</td>\n",
              "      <td>Ski Jumping</td>\n",
              "      <td>Ski Jumping Men's Large Hill, Team</td>\n",
              "      <td>None</td>\n",
              "      <td>27.0*176.0?59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271114</th>\n",
              "      <td>135571</td>\n",
              "      <td>Tomasz Ireneusz ya</td>\n",
              "      <td>M</td>\n",
              "      <td>Poland</td>\n",
              "      <td>POL</td>\n",
              "      <td>1998 Winter</td>\n",
              "      <td>1998</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Nagano</td>\n",
              "      <td>Bobsleigh</td>\n",
              "      <td>Bobsleigh Men's Four</td>\n",
              "      <td>None</td>\n",
              "      <td>30.0(185.0?96.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271115</th>\n",
              "      <td>135571</td>\n",
              "      <td>Tomasz Ireneusz ya</td>\n",
              "      <td>M</td>\n",
              "      <td>Poland</td>\n",
              "      <td>POL</td>\n",
              "      <td>2002 Winter</td>\n",
              "      <td>2002</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Salt Lake City</td>\n",
              "      <td>Bobsleigh</td>\n",
              "      <td>Bobsleigh Men's Four</td>\n",
              "      <td>None</td>\n",
              "      <td>34.0(185.0?96.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>271116 rows √ó 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            ID                      Name Sex            Team  NOC  \\\n",
              "0            1                 A Dijiang   M           China  CHN   \n",
              "1            2                  A Lamusi   M           China  CHN   \n",
              "2            3       Gunnar Nielsen Aaby   M         Denmark  DEN   \n",
              "3            4      Edgar Lindenau Aabye   M  Denmark/Sweden  DEN   \n",
              "4            5  Christine Jacoba Aaftink   F     Netherlands  NED   \n",
              "...        ...                       ...  ..             ...  ...   \n",
              "271111  135569                Andrzej ya   M        Poland-1  POL   \n",
              "271112  135570                  Piotr ya   M          Poland  POL   \n",
              "271113  135570                  Piotr ya   M          Poland  POL   \n",
              "271114  135571        Tomasz Ireneusz ya   M          Poland  POL   \n",
              "271115  135571        Tomasz Ireneusz ya   M          Poland  POL   \n",
              "\n",
              "              Games  Year  Season            City          Sport  \\\n",
              "0       1992 Summer  1992  Summer       Barcelona     Basketball   \n",
              "1       2012 Summer  2012  Summer          London           Judo   \n",
              "2       1920 Summer  1920  Summer       Antwerpen       Football   \n",
              "3       1900 Summer  1900  Summer           Paris     Tug-Of-War   \n",
              "4       1988 Winter  1988  Winter         Calgary  Speed Skating   \n",
              "...             ...   ...     ...             ...            ...   \n",
              "271111  1976 Winter  1976  Winter       Innsbruck           Luge   \n",
              "271112  2014 Winter  2014  Winter           Sochi    Ski Jumping   \n",
              "271113  2014 Winter  2014  Winter           Sochi    Ski Jumping   \n",
              "271114  1998 Winter  1998  Winter          Nagano      Bobsleigh   \n",
              "271115  2002 Winter  2002  Winter  Salt Lake City      Bobsleigh   \n",
              "\n",
              "                                           Event Medal age-height-weight  \n",
              "0                    Basketball Men's Basketball  None   24.0*180.0?80.0  \n",
              "1                   Judo Men's Extra-Lightweight  None   23.0(170.0?60.0  \n",
              "2                        Football Men's Football  None      24.0(nan?nan  \n",
              "3                    Tug-Of-War Men's Tug-Of-War  Gold      34.0:nan?nan  \n",
              "4               Speed Skating Women's 500 metres  None   21.0(185.0?82.0  \n",
              "...                                          ...   ...               ...  \n",
              "271111                Luge Mixed (Men)'s Doubles  None   29.0:179.0?89.0  \n",
              "271112  Ski Jumping Men's Large Hill, Individual  None   27.0:176.0?59.0  \n",
              "271113        Ski Jumping Men's Large Hill, Team  None   27.0*176.0?59.0  \n",
              "271114                      Bobsleigh Men's Four  None   30.0(185.0?96.0  \n",
              "271115                      Bobsleigh Men's Four  None   34.0(185.0?96.0  \n",
              "\n",
              "[271116 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "test = pd.read_parquet('olimpiadas.parquet')\n",
        "display(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 643,
      "metadata": {
        "id": "20qn4zttUPjn"
      },
      "outputs": [],
      "source": [
        "#esta columna extra es para poder probar la funcion de matriz de correlacion.\n",
        "test['num_for_test'] = test['Year'].apply(lambda x: x*np.random.random()).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtxB4EWAy6xY"
      },
      "source": [
        "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
        "\n",
        "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
        "\n",
        "1. Introducci√≥n\n",
        "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
        "\n",
        "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
        "\n",
        "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
        "\n",
        "- Describir la tarea asociada al dataset.\n",
        "- Describir brevemente los datos de entrada que les provee el problema.\n",
        "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
        "\n",
        "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
        "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
        "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
        "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
        "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
        "        - ¬øExisten datos duplicados en el conjunto?\n",
        "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
        "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
        "3. Creaci√≥n de Clusters y Anomal√≠as\n",
        "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
        "    \n",
        "4. An√°lisis de Resultados\n",
        "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
        "    \n",
        "5. Conclusi√≥n\n",
        "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLruh20Zy6xY"
      },
      "source": [
        "## 1. Introducci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLMT7qPXGxyW"
      },
      "source": [
        "Una de las principales tareas en el mundo del Data Science es lograr proveer insights sobre la data que se est√° estudiando. Comunmente, la data puede ser de gran volumen, con muchos datapoints y m√∫ltiples atributos. Para procesar esta data es necesario construir funcionalidades que procesen la data y para esto el uso de programaci√≥n orientada a objetos es la mejor opci√≥n.\n",
        "\n",
        "En esta ocasi√≥n se ha solicitado llevar a cabo un an√°lisis sobre el dataset `'olimpiadas.parquet'`, de manera de obtener los mencionados insights.\n",
        "\n",
        "El dataset provisto cuenta con informaci√≥n acerca de los resultados de los juegos ol√≠mpicos a trav√©s del tiempo. Se mencionan los participantes, su nacionalidad, a√±o de participaci√≥n, temporada (juegos de invierno o verano), ciudad, disciplina, evento, si se ha ganado medalla y detalles acerca de la edad, el peso y la altura del participante.\n",
        "\n",
        "Si bien se podr√≠a construir un perfilador (profiler) que estudiara autom√°ticamente el dataset en espec√≠fico, se ha solicitado que el perfilador sea agn√≥stico al dataset de entrada, siendo  la caracterizaci√≥n autom√°tica, cosa que esta soluci√≥n sea aplicable a cualquier dataset.\n",
        "\n",
        "Para esto, es posible construir una clase de Python que lleve a cabo este procesamiento, siendo agn√≥stica a la data que se le ingresa y provea insights automatizados sobre la data. Esta es la hip√≥tesis que se busca confirmar durante este proyecto.\n",
        "\n",
        "En este proyecto se construy√≥ la clase `Profiler()`, la cual recibe un dataframe y crea un perfil del dataset a trav√©s de sus m√©todos.\n",
        "\n",
        "La clase Profiler() contiene los sigueintes m√©todos:\n",
        "1. `summarize()`: caracteriza las variables del dataset entregado al objeto profiler.\n",
        "2. `plot_vars`: grafica las variables del dataset. Puede graficar todas las columnas del dataset o bien recibir una lista de nombres de las columnas del dataset. El m√©todo graficar√° correlaci√≥n para las variables num√©ricas, cramer V para las variables categ√≥ricas, densidad para las variables num√©ricas y gr√°ficos de frecuencia para las variables categ√≥ricas. El n√∫mero de columnas admitidas en los gr√°ficos de frecuencia es ajustable a trav√©s del par√°metro `N_adj`.\n",
        "3. `clean_data()`: limpia los datos. Es parte de las opciones de preprocesamiento de la clase.\n",
        "4. `scale()`: escala la data, es parte de las opciones de preprocesamiento de la clase.\n",
        "5. `make_clusters()`: lanza una instancia de Kmeans() de sklearn. El n√∫mero de clusters puede ser entregado al algoritmo a trav√©s del par√°metro `n_clusters`. De lo contrario, se asignar√° un n√∫mero √≥ptimo de clusters a trav√©s del m√©todo del codo.\n",
        "6. `detect_anomalies()` detecta anomal√≠as basado en XXXX\n",
        "7. `profile()`: ejecuta todos los m√©todos anteriores.\n",
        "8. `clearGarbage()` elimina todos los archivos generados por profiler.\n",
        "\n",
        "Adem√°s, cuenta con los sigueintes atributos:\n",
        "1. `self.df`: almacena el dataset a perfilar.\n",
        "2. `self.current_date`: almacena la fecha actual para construir el directorio donde se almacenar√°n los resultados del perfilador.\n",
        "3. `self.EDA_directory_name`: directorio donde se almacenan los resultados del m√©todo EDA.\n",
        "4. `self.PLOT__directory`: directorio donde se almacenan los gr√°ficos generados.\n",
        "5. `self.CLEAN_directory_name`: directorio donde se almacenan los datos limpiados.\n",
        "6. `self.CLUSTERS_directory_name`: directorio donde se almacenan los resultados de la generaci√≥n de clusters.\n",
        "7. `self.SCALE_directory_name`: directorio donde se almacenan los resultados del escalamiento.\n",
        "8. `self.categorical`: almacena un dataframe que contiene solo las variables categ√≥ricas del dataframe original.\n",
        "9. `self.numerical`: almacena un dataframe que contiene solo las variables num√©ricas del dataframe original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV5SWiqkGxyX"
      },
      "source": [
        "## 2.An√°lisis EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnLC0hMOGxyX"
      },
      "source": [
        "Cargar la data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 644,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn9hUNDDGxyX",
        "outputId": "3dfcbca9-2daf-42bc-9401-6d8e5b12b2de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID                      Name Sex            Team  NOC        Games  Year  \\\n",
            "0   1                 A Dijiang   M           China  CHN  1992 Summer  1992   \n",
            "1   2                  A Lamusi   M           China  CHN  2012 Summer  2012   \n",
            "2   3       Gunnar Nielsen Aaby   M         Denmark  DEN  1920 Summer  1920   \n",
            "3   4      Edgar Lindenau Aabye   M  Denmark/Sweden  DEN  1900 Summer  1900   \n",
            "4   5  Christine Jacoba Aaftink   F     Netherlands  NED  1988 Winter  1988   \n",
            "\n",
            "   Season       City          Sport                             Event Medal  \\\n",
            "0  Summer  Barcelona     Basketball       Basketball Men's Basketball  None   \n",
            "1  Summer     London           Judo      Judo Men's Extra-Lightweight  None   \n",
            "2  Summer  Antwerpen       Football           Football Men's Football  None   \n",
            "3  Summer      Paris     Tug-Of-War       Tug-Of-War Men's Tug-Of-War  Gold   \n",
            "4  Winter    Calgary  Speed Skating  Speed Skating Women's 500 metres  None   \n",
            "\n",
            "  age-height-weight  \n",
            "0   24.0*180.0?80.0  \n",
            "1   23.0(170.0?60.0  \n",
            "2      24.0(nan?nan  \n",
            "3      34.0:nan?nan  \n",
            "4   21.0(185.0?82.0  \n"
          ]
        }
      ],
      "source": [
        "test = pd.read_parquet('olimpiadas.parquet')[:10000]\n",
        "print(test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 645,
      "metadata": {
        "id": "kxKS7SQNGxyX"
      },
      "outputs": [],
      "source": [
        "#esta columna extra es para poder probar la funcion de matriz de correlacion.\n",
        "test['num_for_test'] = test['Year'].apply(lambda x: x*np.random.random()).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUuDaNyWGxyY"
      },
      "source": [
        "Incializar el profiler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 646,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxHL0w6KGxyY",
        "outputId": "cb2ec115-1fa5-4a4f-c18f-4dabb58759fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory 'EDA_06-11-2023' created (overwritten) successfully.\n"
          ]
        }
      ],
      "source": [
        "test_class = Profiler(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuQwF5dIGxyY"
      },
      "source": [
        "Es posible acceder al m√©todo `.describe()` del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 647,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "B-WwJoXHGxyY",
        "outputId": "2cd72c14-1970-40b5-9099-d289d4fc7430"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Year</th>\n",
              "      <th>num_for_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10000.00</td>\n",
              "      <td>10000.00</td>\n",
              "      <td>10000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2789.48</td>\n",
              "      <td>1980.48</td>\n",
              "      <td>987.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1588.32</td>\n",
              "      <td>28.61</td>\n",
              "      <td>571.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1896.00</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1405.75</td>\n",
              "      <td>1964.00</td>\n",
              "      <td>493.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2851.00</td>\n",
              "      <td>1988.00</td>\n",
              "      <td>989.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4190.25</td>\n",
              "      <td>2004.00</td>\n",
              "      <td>1482.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5463.00</td>\n",
              "      <td>2016.00</td>\n",
              "      <td>2015.21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             ID      Year  num_for_test\n",
              "count  10000.00  10000.00      10000.00\n",
              "mean    2789.48   1980.48        987.25\n",
              "std     1588.32     28.61        571.42\n",
              "min        1.00   1896.00          0.04\n",
              "25%     1405.75   1964.00        493.19\n",
              "50%     2851.00   1988.00        989.90\n",
              "75%     4190.25   2004.00       1482.04\n",
              "max     5463.00   2016.00       2015.21"
            ]
          },
          "execution_count": 647,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_class.df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJPVdD1CGxyY"
      },
      "source": [
        "En todos los datasets es de inter√©s estudiar las variables de este. Tanto las num√©ricas como las categ√≥ricas. Con la funcion `summarize()` se puede construir un resumen de las variables del dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 648,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHycIPWMGxyZ",
        "outputId": "cbdeab30-ec30-4db2-ca95-b5e8cf398edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary.txt has been successfully exported too EDA_06-11-2023\n",
            "\n",
            "------------------------------------------------------------\n",
            "Los tipos de las variables son: \n",
            "ID: int64\n",
            "Name: object\n",
            "Sex: object\n",
            "Team: object\n",
            "NOC: object\n",
            "Games: object\n",
            "Year: int64\n",
            "Season: object\n",
            "City: object\n",
            "Sport: object\n",
            "Event: object\n",
            "Medal: object\n",
            "age-height-weight: object\n",
            "num_for_test: float64\n",
            "------------------------------------------------------------\n",
            "Cantidad de NaNs por variable: \n",
            "ID: 0\n",
            "Name: 0\n",
            "Sex: 0\n",
            "Team: 0\n",
            "NOC: 0\n",
            "Games: 0\n",
            "Year: 0\n",
            "Season: 0\n",
            "City: 0\n",
            "Sport: 0\n",
            "Event: 0\n",
            "Medal: 8775\n",
            "age-height-weight: 0\n",
            "num_for_test: 0\n",
            "------------------------------------------------------------\n",
            "Cantidad de valores √É¬∫nicos por variable: \n",
            "ID: 5463\n",
            "Name: 5446\n",
            "Sex: 2\n",
            "Team: 307\n",
            "NOC: 193\n",
            "Games: 51\n",
            "Year: 35\n",
            "Season: 2\n",
            "City: 42\n",
            "Sport: 57\n",
            "Event: 588\n",
            "Medal: 3\n",
            "age-height-weight: 6830\n",
            "num_for_test: 9743\n",
            "------------------------------------------------------------\n",
            "Cantidad de ceros en las variables numericas: \n",
            "ID: 0\n",
            "Year: 0\n",
            "num_for_test: 0\n",
            "------------------------------------------------------------\n",
            "Promedios de las variables numericas: \n",
            "ID: 2789.4804\n",
            "Year: 1980.4768\n",
            "num_for_test: 987.252006\n",
            "------------------------------------------------------------\n",
            "Cantidad de negativos de las variables numericas: \n",
            "ID: 0\n",
            "Year: 0\n",
            "num_for_test: 0\n",
            "------------------------------------------------------------\n",
            "Maximos de las variables numericas: \n",
            "ID: 5463.0\n",
            "Year: 2016.0\n",
            "num_for_test: 2015.21\n",
            "------------------------------------------------------------\n",
            "M√É¬≠nimos de las variables numericas: \n",
            "ID: 1.0\n",
            "Year: 1896.0\n",
            "num_for_test: 0.04\n",
            "------------------------------------------------------------\n",
            "Outliers de las variables numericas: \n",
            "ID: 0\n",
            "Year: 45\n",
            "num_for_test: 0\n",
            "------------------------------------------------------------\n",
            "Percentil 25.0 de las variables numericas: \n",
            "ID: 1405.75\n",
            "Year: 1964.0\n",
            "num_for_test: 493.19000000000005\n",
            "------------------------------------------------------------\n",
            "Percentil 50.0 de las variables numericas: \n",
            "ID: 2851.0\n",
            "Year: 1988.0\n",
            "num_for_test: 989.9\n",
            "------------------------------------------------------------\n",
            "Percentil 75.0 de las variables numericas: \n",
            "ID: 4190.25\n",
            "Year: 2004.0\n",
            "num_for_test: 1482.0375\n",
            "------------------------------------------------------------\n",
            "Percentil 100.0 de las variables numericas: \n",
            "ID: 5463.0\n",
            "Year: 2016.0\n",
            "num_for_test: 2015.21\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_class.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNQVxO0PGxyZ"
      },
      "source": [
        "**¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?**\n",
        ">\n",
        "\n",
        "**¬øExisten datos duplicados en el conjunto?**\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKdZklfrGxyZ"
      },
      "source": [
        "Sin embargo, tambi√©n se pueden construir visualizaci√≥nes de las variables con el m√©todo `plot_vars()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 649,
      "metadata": {
        "id": "hhAluRirGxyZ"
      },
      "outputs": [],
      "source": [
        "#test_class.plot_vars()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjiMlTX6GxyZ"
      },
      "source": [
        "**¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?**\n",
        "> Para las variables num√©ricas, se observa una matriz de correlaci√≥n. No se observan correlaciones num√©ricas relevantes para el an√°lisis.\n",
        "\n",
        "\n",
        "**¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?**\n",
        "        \n",
        "**¬øExisten relaciones o patrones visuales entre las variables?**\n",
        ">\n",
        "**¬øExisten anomal√≠as notables o preocupantes en los datos?**\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZg-YVqvGxya"
      },
      "source": [
        "## 3. Clusters y anomal√≠as."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 650,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mdV0EvqGxya",
        "outputId": "a388aa8c-5fcc-43f3-e2df-ba64a8e2954c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory 'EDA_06-11-2023/clean_data' created (overwritten) successfully.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Team</th>\n",
              "      <th>NOC</th>\n",
              "      <th>Games</th>\n",
              "      <th>Year</th>\n",
              "      <th>Season</th>\n",
              "      <th>City</th>\n",
              "      <th>Sport</th>\n",
              "      <th>Event</th>\n",
              "      <th>Medal</th>\n",
              "      <th>num_for_test</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, Name, Sex, Team, NOC, Games, Year, Season, City, Sport, Event, Medal, num_for_test, age, height, weight]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "porcentaje de data retenida:  0.0\n"
          ]
        }
      ],
      "source": [
        "test_class.clean_data(non_atomic=[\"age-height-weight\"],column_name=test.columns,everything=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alQjUXLcSE-g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUrmcN70SFCO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 651,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "cYwlJNRMGxya",
        "outputId": "caa8e617-f3b9-428e-e161-af0dce908d12"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Team</th>\n",
              "      <th>NOC</th>\n",
              "      <th>Games</th>\n",
              "      <th>Year</th>\n",
              "      <th>Season</th>\n",
              "      <th>City</th>\n",
              "      <th>Sport</th>\n",
              "      <th>Event</th>\n",
              "      <th>Medal</th>\n",
              "      <th>age-height-weight</th>\n",
              "      <th>num_for_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A Dijiang</td>\n",
              "      <td>M</td>\n",
              "      <td>China</td>\n",
              "      <td>CHN</td>\n",
              "      <td>1992 Summer</td>\n",
              "      <td>1992</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Barcelona</td>\n",
              "      <td>Basketball</td>\n",
              "      <td>Basketball Men's Basketball</td>\n",
              "      <td>None</td>\n",
              "      <td>24.0*180.0?80.0</td>\n",
              "      <td>394.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A Lamusi</td>\n",
              "      <td>M</td>\n",
              "      <td>China</td>\n",
              "      <td>CHN</td>\n",
              "      <td>2012 Summer</td>\n",
              "      <td>2012</td>\n",
              "      <td>Summer</td>\n",
              "      <td>London</td>\n",
              "      <td>Judo</td>\n",
              "      <td>Judo Men's Extra-Lightweight</td>\n",
              "      <td>None</td>\n",
              "      <td>23.0(170.0?60.0</td>\n",
              "      <td>507.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Gunnar Nielsen Aaby</td>\n",
              "      <td>M</td>\n",
              "      <td>Denmark</td>\n",
              "      <td>DEN</td>\n",
              "      <td>1920 Summer</td>\n",
              "      <td>1920</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Antwerpen</td>\n",
              "      <td>Football</td>\n",
              "      <td>Football Men's Football</td>\n",
              "      <td>None</td>\n",
              "      <td>24.0(nan?nan</td>\n",
              "      <td>1803.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Edgar Lindenau Aabye</td>\n",
              "      <td>M</td>\n",
              "      <td>Denmark/Sweden</td>\n",
              "      <td>DEN</td>\n",
              "      <td>1900 Summer</td>\n",
              "      <td>1900</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Paris</td>\n",
              "      <td>Tug-Of-War</td>\n",
              "      <td>Tug-Of-War Men's Tug-Of-War</td>\n",
              "      <td>Gold</td>\n",
              "      <td>34.0:nan?nan</td>\n",
              "      <td>1180.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Christine Jacoba Aaftink</td>\n",
              "      <td>F</td>\n",
              "      <td>Netherlands</td>\n",
              "      <td>NED</td>\n",
              "      <td>1988 Winter</td>\n",
              "      <td>1988</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Calgary</td>\n",
              "      <td>Speed Skating</td>\n",
              "      <td>Speed Skating Women's 500 metres</td>\n",
              "      <td>None</td>\n",
              "      <td>21.0(185.0?82.0</td>\n",
              "      <td>1178.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>5460</td>\n",
              "      <td>Kiyoshi Asai</td>\n",
              "      <td>M</td>\n",
              "      <td>Japan</td>\n",
              "      <td>JPN</td>\n",
              "      <td>1964 Summer</td>\n",
              "      <td>1964</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Tokyo</td>\n",
              "      <td>Athletics</td>\n",
              "      <td>Athletics Men's 4 x 100 metres Relay</td>\n",
              "      <td>None</td>\n",
              "      <td>24.0:167.0?60.0</td>\n",
              "      <td>976.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>5461</td>\n",
              "      <td>Tadashi Asai</td>\n",
              "      <td>M</td>\n",
              "      <td>Japan</td>\n",
              "      <td>JPN</td>\n",
              "      <td>1956 Summer</td>\n",
              "      <td>1956</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Melbourne</td>\n",
              "      <td>Wrestling</td>\n",
              "      <td>Wrestling Men's Flyweight, Freestyle</td>\n",
              "      <td>None</td>\n",
              "      <td>20.0:162.0?57.0</td>\n",
              "      <td>504.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>5461</td>\n",
              "      <td>Tadashi Asai</td>\n",
              "      <td>M</td>\n",
              "      <td>Japan</td>\n",
              "      <td>JPN</td>\n",
              "      <td>1960 Summer</td>\n",
              "      <td>1960</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Roma</td>\n",
              "      <td>Wrestling</td>\n",
              "      <td>Wrestling Men's Bantamweight, Freestyle</td>\n",
              "      <td>None</td>\n",
              "      <td>24.0*162.0?57.0</td>\n",
              "      <td>1341.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>5462</td>\n",
              "      <td>Yoshiro Asakuma</td>\n",
              "      <td>M</td>\n",
              "      <td>Japan</td>\n",
              "      <td>JPN</td>\n",
              "      <td>1936 Summer</td>\n",
              "      <td>1936</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>Athletics</td>\n",
              "      <td>Athletics Men's High Jump</td>\n",
              "      <td>None</td>\n",
              "      <td>22.0*180.0?65.0</td>\n",
              "      <td>78.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>5463</td>\n",
              "      <td>Tony J. Asamali</td>\n",
              "      <td>M</td>\n",
              "      <td>Philippines</td>\n",
              "      <td>PHI</td>\n",
              "      <td>1968 Summer</td>\n",
              "      <td>1968</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Mexico City</td>\n",
              "      <td>Swimming</td>\n",
              "      <td>Swimming Men's 200 metres Freestyle</td>\n",
              "      <td>None</td>\n",
              "      <td>21.0(193.0?64.0</td>\n",
              "      <td>177.56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows √ó 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID                      Name Sex            Team  NOC        Games  \\\n",
              "0        1                 A Dijiang   M           China  CHN  1992 Summer   \n",
              "1        2                  A Lamusi   M           China  CHN  2012 Summer   \n",
              "2        3       Gunnar Nielsen Aaby   M         Denmark  DEN  1920 Summer   \n",
              "3        4      Edgar Lindenau Aabye   M  Denmark/Sweden  DEN  1900 Summer   \n",
              "4        5  Christine Jacoba Aaftink   F     Netherlands  NED  1988 Winter   \n",
              "...    ...                       ...  ..             ...  ...          ...   \n",
              "9995  5460              Kiyoshi Asai   M           Japan  JPN  1964 Summer   \n",
              "9996  5461              Tadashi Asai   M           Japan  JPN  1956 Summer   \n",
              "9997  5461              Tadashi Asai   M           Japan  JPN  1960 Summer   \n",
              "9998  5462           Yoshiro Asakuma   M           Japan  JPN  1936 Summer   \n",
              "9999  5463           Tony J. Asamali   M     Philippines  PHI  1968 Summer   \n",
              "\n",
              "      Year  Season         City          Sport  \\\n",
              "0     1992  Summer    Barcelona     Basketball   \n",
              "1     2012  Summer       London           Judo   \n",
              "2     1920  Summer    Antwerpen       Football   \n",
              "3     1900  Summer        Paris     Tug-Of-War   \n",
              "4     1988  Winter      Calgary  Speed Skating   \n",
              "...    ...     ...          ...            ...   \n",
              "9995  1964  Summer        Tokyo      Athletics   \n",
              "9996  1956  Summer    Melbourne      Wrestling   \n",
              "9997  1960  Summer         Roma      Wrestling   \n",
              "9998  1936  Summer       Berlin      Athletics   \n",
              "9999  1968  Summer  Mexico City       Swimming   \n",
              "\n",
              "                                        Event Medal age-height-weight  \\\n",
              "0                 Basketball Men's Basketball  None   24.0*180.0?80.0   \n",
              "1                Judo Men's Extra-Lightweight  None   23.0(170.0?60.0   \n",
              "2                     Football Men's Football  None      24.0(nan?nan   \n",
              "3                 Tug-Of-War Men's Tug-Of-War  Gold      34.0:nan?nan   \n",
              "4            Speed Skating Women's 500 metres  None   21.0(185.0?82.0   \n",
              "...                                       ...   ...               ...   \n",
              "9995     Athletics Men's 4 x 100 metres Relay  None   24.0:167.0?60.0   \n",
              "9996     Wrestling Men's Flyweight, Freestyle  None   20.0:162.0?57.0   \n",
              "9997  Wrestling Men's Bantamweight, Freestyle  None   24.0*162.0?57.0   \n",
              "9998                Athletics Men's High Jump  None   22.0*180.0?65.0   \n",
              "9999      Swimming Men's 200 metres Freestyle  None   21.0(193.0?64.0   \n",
              "\n",
              "      num_for_test  \n",
              "0           394.32  \n",
              "1           507.50  \n",
              "2          1803.43  \n",
              "3          1180.89  \n",
              "4          1178.67  \n",
              "...            ...  \n",
              "9995        976.40  \n",
              "9996        504.21  \n",
              "9997       1341.02  \n",
              "9998         78.46  \n",
              "9999        177.56  \n",
              "\n",
              "[10000 rows x 14 columns]"
            ]
          },
          "execution_count": 651,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_class.df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 652,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhQeAuaOGxya",
        "outputId": "2331109a-c556-4bee-c3b2-b75a6ef2e2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory 'EDA_06-11-2023/scaled' created (overwritten) successfully.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\magda\\OneDrive\\Escritorio\\SCRIPTS_VARIOS\\LabMDS-1\\Proyecto_1\\NUEVO_Proyecto_enunciado.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y320sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_class\u001b[39m.\u001b[39mscale()\n",
            "\u001b[1;32mc:\\Users\\magda\\OneDrive\\Escritorio\\SCRIPTS_VARIOS\\LabMDS-1\\Proyecto_1\\NUEVO_Proyecto_enunciado.ipynb Cell 33\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y320sZmlsZQ%3D%3D?line=426'>427</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y320sZmlsZQ%3D%3D?line=427'>428</a>\u001b[0m     preprocessor \u001b[39m=\u001b[39m ColumnTransformer(transformers\u001b[39m=\u001b[39m[(\u001b[39m'\u001b[39m\u001b[39mvar_num\u001b[39m\u001b[39m'\u001b[39m, numericas_transf, numericas)])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y320sZmlsZQ%3D%3D?line=428'>429</a>\u001b[0m data_procesada \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39;49mfit_transform(clean_data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y320sZmlsZQ%3D%3D?line=429'>430</a>\u001b[0m df_final\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data_procesada)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y320sZmlsZQ%3D%3D?line=430'>431</a>\u001b[0m df_final\u001b[39m.\u001b[39mto_csv(file_path)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:690\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    688\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m--> 690\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, y, _fit_transform_one)\n\u001b[0;32m    692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[0;32m    693\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fitted_transformers([])\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:621\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    615\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    616\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[0;32m    617\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    619\u001b[0m )\n\u001b[0;32m    620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    622\u001b[0m         delayed(func)(\n\u001b[0;32m    623\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[0;32m    624\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    625\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    626\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m    627\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    628\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[0;32m    629\u001b[0m         )\n\u001b[0;32m    630\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    631\u001b[0m     )\n\u001b[0;32m    632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    633\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:422\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    420\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit_transform(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:855\u001b[0m, in \u001b[0;36mOneHotEncoder.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[39mFit OneHotEncoder to X, then transform X.\u001b[39;00m\n\u001b[0;32m    835\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39m    returned.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_keywords()\n\u001b[1;32m--> 855\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(X, y)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:818\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39mFit OneHotEncoder to X.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39m    Fitted encoder.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_keywords()\n\u001b[1;32m--> 818\u001b[0m fit_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    819\u001b[0m     X,\n\u001b[0;32m    820\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[0;32m    821\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    822\u001b[0m     return_counts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infrequent_enabled,\n\u001b[0;32m    823\u001b[0m )\n\u001b[0;32m    824\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infrequent_enabled:\n\u001b[0;32m    825\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_infrequent_category_mapping(\n\u001b[0;32m    826\u001b[0m         fit_results[\u001b[39m\"\u001b[39m\u001b[39mn_samples\u001b[39m\u001b[39m\"\u001b[39m], fit_results[\u001b[39m\"\u001b[39m\u001b[39mcategory_counts\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    827\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:80\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 80\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[0;32m     81\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:61\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_features):\n\u001b[0;32m     60\u001b[0m     Xi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_feature(X, feature_idx\u001b[39m=\u001b[39mi)\n\u001b[1;32m---> 61\u001b[0m     Xi \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m     62\u001b[0m         Xi, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mneeds_validation\n\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     64\u001b[0m     X_columns\u001b[39m.\u001b[39mappend(Xi)\n\u001b[0;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m X_columns, n_samples, n_features\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:909\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 909\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    910\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    915\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    916\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
            "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
          ]
        }
      ],
      "source": [
        "test_class.scale()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaudiXfvdyk7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "1SJ0HW35Gxya",
        "outputId": "20ff4726-fffd-48d5-cf5a-7079c2491c01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory 'EDA_06-11-2023/clusters' created (overwritten) successfully.\n",
            "n_clusters is set to None. n_clusters will be initialized via the internal auto_elbow_method function.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "testing clusters in elbow method:   0%|          | 0/8 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'A Dijiang'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\magda\\OneDrive\\Escritorio\\SCRIPTS_VARIOS\\LabMDS-1\\Proyecto_1\\NUEVO_Proyecto_enunciado.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_class\u001b[39m.\u001b[39mmake_clusters(kmeans)\n",
            "\u001b[1;32mc:\\Users\\magda\\OneDrive\\Escritorio\\SCRIPTS_VARIOS\\LabMDS-1\\Proyecto_1\\NUEVO_Proyecto_enunciado.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=510'>511</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(clustering_algorithm,KMeans) \u001b[39mand\u001b[39;00m n_clusters \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=511'>512</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_clusters is set to None. n_clusters will be initialized via the internal auto_elbow_method function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=512'>513</a>\u001b[0m     n_clusters \u001b[39m=\u001b[39m  auto_elbow_method(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=513'>514</a>\u001b[0m     clustering_algorithm\u001b[39m.\u001b[39mset_params(n_clusters \u001b[39m=\u001b[39m n_clusters)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=514'>515</a>\u001b[0m \u001b[39m# Create a pipeline with PCA and KMeans\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\magda\\OneDrive\\Escritorio\\SCRIPTS_VARIOS\\LabMDS-1\\Proyecto_1\\NUEVO_Proyecto_enunciado.ipynb Cell 35\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=490'>491</a>\u001b[0m \u001b[39mfor\u001b[39;00m n_clusters \u001b[39min\u001b[39;00m tqdm(n_clusters_range,desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtesting clusters in elbow method\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=491'>492</a>\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39mn_clusters,n_init\u001b[39m=\u001b[39m_n_init)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=492'>493</a>\u001b[0m     kmeans\u001b[39m.\u001b[39;49mfit(data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=493'>494</a>\u001b[0m     inertia\u001b[39m.\u001b[39mappend(kmeans\u001b[39m.\u001b[39minertia_)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=495'>496</a>\u001b[0m \u001b[39mif\u001b[39;00m plot_elbow:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Escritorio/SCRIPTS_VARIOS/LabMDS-1/Proyecto_1/NUEVO_Proyecto_enunciado.ipynb#Y322sZmlsZQ%3D%3D?line=496'>497</a>\u001b[0m     \u001b[39m# Create the Elbow Method graph\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1367\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1342\u001b[0m     \u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1343\u001b[0m \n\u001b[0;32m   1344\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1367\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1368\u001b[0m         X,\n\u001b[0;32m   1369\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1370\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[0;32m   1371\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1372\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[0;32m   1373\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1374\u001b[0m     )\n\u001b[0;32m   1376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(X)\n\u001b[0;32m   1377\u001b[0m     random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:2069\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'A Dijiang'"
          ]
        }
      ],
      "source": [
        "kmeans = KMeans()\n",
        "test_class.make_clusters(kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NmQU0emGxyb"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=0.2, min_samples=10)\n",
        "test_class.make_clusters(dbscan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y09epWSFGxyb"
      },
      "outputs": [],
      "source": [
        "agg= AgglomerativeClustering()\n",
        "test_class.make_clusters(agg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk05k-JRGxyb"
      },
      "source": [
        "## 4.Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq9BYZ3DGxyb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whQicjzlGxyb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qIT4lAOGxyb"
      },
      "source": [
        "## 5. Conclusi√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzEqo8mQGxyc"
      },
      "source": [
        "En este proyecto se ha visto la efectividad de construir una clase perfiladora agn√≥stica al dataset, puesto que ahora ser√° posible ejecutar m√∫ltiples EDAs sobre cualquier dataset de manera r√°pida y eficaz."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vk05k-JRGxyb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
