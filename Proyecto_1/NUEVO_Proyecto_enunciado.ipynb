{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vTs1ax9y6xN"
      },
      "source": [
        "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE5KA3OEy6xQ"
      },
      "source": [
        "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
        "\n",
        "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesor: Ignacio Meza, Gabriel Iturra\n",
        "- Auxiliar: Sebasti√°n Tinoco\n",
        "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
        "\n",
        "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
        "\n",
        "### Equipo:\n",
        "\n",
        "- Simon Repolt\n",
        "- Magdalena de la Fuente\n",
        "\n",
        "\n",
        "### Link de repositorio de GitHub: `https://github.com/HexaPulsar/LabMDS`\n",
        "\n",
        "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLU9ydRby6xQ"
      },
      "source": [
        "----\n",
        "\n",
        "## Reglas\n",
        "\n",
        "- **Grupos de 2 personas.**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Estrictamente prohibida la copia.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNpMtVMZy6xR"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
        "</div>\n",
        "\n",
        "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
        "\n",
        "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
        "- Caracterizaci√≥n autom√°tica de los datos\n",
        "- La soluci√≥n debe ser compatible con cualquier dataset\n",
        "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMutfXyfy6xR"
      },
      "source": [
        "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
        "\n",
        "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
        "\n",
        "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
        "\n",
        "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Reportar el tipo de variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
        "    - Si la variables es num√©rica:\n",
        "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
        "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
        "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
        "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
        "\n",
        "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/plots`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Para las variables num√©ricas:\n",
        "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
        "        - Grafique la correlaci√≥n entre las variables\n",
        "    - Para las variables categ√≥ricas:\n",
        "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
        "        - Grafique el coeficiente V de Cramer entre las variables\n",
        "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
        "    \n",
        "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clean_data`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Drop de valores duplicados\n",
        "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
        "        - Drop de valores nulos\n",
        "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
        "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
        "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
        "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
        "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
        "    - Deber√≠an usar `FunctionTransformer`.\n",
        "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
        "\n",
        "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/scale`\n",
        "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
        "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
        "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
        "        - Asuma que no existen datos ordinales en su dataset\n",
        "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
        "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
        "\n",
        "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clusters`\n",
        "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
        "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
        "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering.\n",
        "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
        "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "\n",
        "    - Crear la carpeta `EDA_fecha/anomalies`\n",
        "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
        "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
        "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a.\n",
        "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
        "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
        "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
        "\n",
        "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
        "\n",
        "Algunas consideraciones generales:\n",
        "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset.\n",
        "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
        "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
        "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh8A54h0y6xT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import shutil\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler,FunctionTransformer,OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "from sklearn.base import is_classifier,is_outlier_detector \n",
        "import re \n",
        "from sklearn.ensemble import IsolationForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEBbL02SWhp3"
      },
      "outputs": [],
      "source": [
        "#TODO En algunas funciones falta agregar prints diciendo que ha ocurrido durante la funcion\n",
        "#\n",
        "#\n",
        "#TODO asegurarse de que los requirements esten correctos antes de entregar y que el pip freeze este al dia\n",
        "#TODO documentar los metodos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWG_VNX4y6xV"
      },
      "outputs": [],
      "source": [
        "#pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrDhWdwky6xW"
      },
      "outputs": [],
      "source": [
        "class Profiler():\n",
        "\n",
        "    \"\"\" la clase profiler recibira un dataframe de pandas en el constructor\n",
        "    La funci√≥n summarize recibe una LISTA de strings de los nombres de las variables (nombre de las columnas del pandas) a las cuales se desea verle su\n",
        "    informacion respectiva\n",
        "    \"\"\"\n",
        "    def __init__(self,df:pd.DataFrame) -> None:\n",
        "        self.df = df\n",
        "        self.current_date = datetime.datetime.now().strftime(f\"%d-%m-%Y\")\n",
        "\n",
        "        self.EDA_directory_name = f\"EDA_{self.current_date}\"\n",
        "        self.PLOT_directory_name = f\"{self.EDA_directory_name}/plots\"\n",
        "        self.CLEAN_directory_name = f\"{self.EDA_directory_name}/clean_data\"\n",
        "        self.CLUSTERS_directory_name = f\"{self.EDA_directory_name}/clusters\"\n",
        "        self.SCALE_directory_name = f\"{self.EDA_directory_name}/scaled\"\n",
        "        self.ANOMALY_directory_name = f\"{self.EDA_directory_name}/anomalies\"\n",
        "        \n",
        "        self.categorical = self.df.select_dtypes(include=['object'])\n",
        "        self.numerical = self.df.select_dtypes(include=['number'])\n",
        "        \n",
        "        try:\n",
        "            if os.path.exists(self.EDA_directory_name):\n",
        "                shutil.rmtree(self.EDA_directory_name)\n",
        "            os.mkdir(self.EDA_directory_name)\n",
        "            print(f\"Directory '{self.EDA_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "    def summarize(self,print_summarize = True,column_name:str or list=None):\n",
        "        \"\"\"summarize: recibe  un string con el nombre de una columna o una lista que contenga el nombre de las columnas a realizar resumen\n",
        "        Si se desea realizar un resumen al dataframe completo dejar la variable everything como True y column_name como None.\n",
        "        En caso de que se desee realizar un resumen por variable(s) seleccionada(s), dejar everything en falso e indicar la o las variables\n",
        "        en el formato ya dicho.\n",
        "\n",
        "        Esta funci√≥n provee un txt guardado en la carpeta EDA_fecha con informacion del dataset o las variables, segun corresponda\n",
        "\n",
        "        Raises:\n",
        "            Exception: _description_\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        def detect_outliers(column):\n",
        "            Q1 = column.quantile(0.25)\n",
        "            Q3 = column.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = (column < lower_bound) | (column > upper_bound)\n",
        "            return outliers.sum()\n",
        "\n",
        "        file_path = f\"{self.EDA_directory_name}/summary.txt\"\n",
        "\n",
        "        def report_txt(vars):\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "\n",
        "                    tipos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.dtypes))\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    f.write(\"Los tipos de las variables son: \"+\"\\n\")\n",
        "                    f.write(tipos)\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "\n",
        "                    nan_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.isna().sum().to_list()))\n",
        "                    f.write(\"Cantidad de NaNs por variable: \"+\"\\n\")\n",
        "                    f.write(nan_values)\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "\n",
        "                    unique_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.nunique().to_list()))\n",
        "                    f.write(\"Cantidad de valores √∫nicos por variable: \"+\"\\n\")\n",
        "                    f.write(unique_values)\n",
        "                    if self.numerical.shape[0] > 1: #si no hay numericas no hacer nada de esto\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        nceros = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, (self.numerical == 0).sum().to_list()))\n",
        "                        f.write(\"Cantidad de ceros en las variables numericas: \"+\"\\n\")\n",
        "                        f.write(nceros)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        promedios =  '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.mean().to_list()))\n",
        "                        f.write(\"Promedios de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(promedios)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        negativos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, (self.numerical < 0).sum().to_list()))\n",
        "                        f.write(\"Cantidad de negativos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(negativos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        maximos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.max().to_list()))\n",
        "                        f.write(\"Maximos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(maximos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        minimos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.min().to_list()))\n",
        "                        f.write(\"M√≠nimos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(minimos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        outliers = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.apply(detect_outliers)))\n",
        "                        f.write(\"Outliers de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(outliers)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        values_perc = [0.25, 0.5, 0.75, 1.0]\n",
        "                        percentiles = self.numerical.quantile(values_perc).values\n",
        "                        for idx,perc in enumerate(values_perc):\n",
        "                            f.write(\"Percentil \" + str(perc*100) + \" de las variables numericas: \"+\"\\n\")\n",
        "                            perc_val = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, percentiles[idx]))\n",
        "                            f.write(perc_val)\n",
        "                            f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    f.close()\n",
        "                    print(f\"Summary.txt has been successfully exported too {self.EDA_directory_name}\")\n",
        "            if print_summarize:\n",
        "                with open(file_path, 'r') as file:\n",
        "                    file_content = file.read()\n",
        "                    print(file_content)\n",
        "\n",
        "        try:\n",
        "            if column_name is None:\n",
        "              report_txt(self.df)\n",
        "\n",
        "            elif isinstance(column_name, str):\n",
        "                var = self.df[[column_name]]\n",
        "                report_txt(var)\n",
        "\n",
        "            elif isinstance(column_name,list):\n",
        "                if not column_name:\n",
        "                    raise ValueError(\"The input columns list is empty\")\n",
        "                vars =  self.df[column_name]\n",
        "                report_txt(vars)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Exception Error: {e}')\n",
        "\n",
        "\n",
        "    def plot_vars(self,column_name= None,N_adj = 10):\n",
        "        \"\"\"\n",
        "        Plots various visualizations for the specified column(s) in the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            column_name (str or list): The name of the column or a list of column names to visualize.\n",
        "            N_adj (int, optional): The number of top categories to consider in histograms. Defaults to 10.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the input columns list is empty.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.PLOT_directory_name):\n",
        "                shutil.rmtree(self.PLOT_directory_name)\n",
        "            os.mkdir(self.PLOT_directory_name)\n",
        "            print(f\"Directory '{self.PLOT_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        def correlation_plot(variables:list):\n",
        "\n",
        "            \"\"\"\n",
        "            Plots a correlation matrix heatmap for a list of numerical variables.\n",
        "\n",
        "            Args:\n",
        "                variables (list): A list of column names representing numerical variables.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "\n",
        "            #TODO revisar que los elementos de la lista sean todos str try/except\n",
        "            correlation_matrix = np.corrcoef(self.numerical[variables], rowvar=False)\n",
        "            plt.figure()\n",
        "            sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt='.2f', xticklabels=variables, yticklabels=variables)\n",
        "            plt.title('Correlation Matrix')\n",
        "\n",
        "\n",
        "        def histogram(var: pd.Series, N: int):\n",
        "            \"\"\"\n",
        "            Plots a histogram for a categorical variable.\n",
        "\n",
        "            Args:\n",
        "                var (pd.Series): A pandas Series representing a categorical variable.\n",
        "                N (int): The number of top categories to display in the histogram.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "            category_counts = var.value_counts()\n",
        "            top_N_categories = category_counts.head(N)\n",
        "            plt.bar(top_N_categories.index, top_N_categories.values)\n",
        "            plt.xlabel('Category')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.title(f'Top {N} Categories Histogram for variable {var.name}')\n",
        "\n",
        "        def cramers_v(confusion_matrix):\n",
        "            \"\"\"\n",
        "            Calculates Cramer's V statistic for two categorical variables.\n",
        "\n",
        "            Args:\n",
        "                confusion_matrix (pd.DataFrame): The contingency table of two categorical variables.\n",
        "\n",
        "            Returns:\n",
        "                float: The Cramer's V statistic.\n",
        "\n",
        "            Notes:\n",
        "                Cramer's V is a measure of association for categorical variables.\n",
        "\n",
        "            \"\"\"\n",
        "            chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "            n = confusion_matrix.sum().sum()\n",
        "            phi2 = chi2 / n\n",
        "            r, k = confusion_matrix.shape\n",
        "            phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
        "            rcorr = r - ((r - 1)**2) / (n - 1)\n",
        "            kcorr = k - ((k - 1)**2) / (n - 1)\n",
        "            return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "        def cramer_v_plot(variables):\n",
        "            \"\"\"\n",
        "            Plots a heatmap of Cramer's V values for all pairs of categorical variables.\n",
        "\n",
        "            Args:\n",
        "                variables (list): A list of column names representing categorical variables.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "            categorical_cols = self.categorical[variables]\n",
        "            results = pd.DataFrame(index=categorical_cols.columns, columns=categorical_cols.columns, dtype=float)\n",
        "            for col1 in categorical_cols.columns:\n",
        "                for col2 in categorical_cols.columns:\n",
        "                    confusion_matrix = pd.crosstab(categorical_cols[col1], categorical_cols[col2])\n",
        "                    results.loc[col1, col2] = cramers_v(confusion_matrix)\n",
        "            plt.figure()\n",
        "            sns.heatmap(results, annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
        "            plt.title(\"Cramer's V Heatmap\")\n",
        "\n",
        "        try:\n",
        "            if column_name is None:\n",
        "                present_num = self.numerical\n",
        "                present_cat = self.categorical\n",
        "\n",
        "                if len(present_num) > 1:\n",
        "                    correlation_plot(present_num.columns)\n",
        "                    plt.gcf()\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/corr.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                if len(present_cat) > 1:\n",
        "                    cramer_v_plot(present_cat.columns)\n",
        "                    plt.gcf()\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/cramer_v.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                for col in self.df.columns:\n",
        "                    if col in self.numerical.columns:\n",
        "                        var = self.df[col]\n",
        "                        var.plot.density(title = f\"Plot de densidad para la variable {col}\")\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "                    if col in self.categorical.columns:\n",
        "                        histogram(self.df[col],N= N_adj)\n",
        "                        plt.gcf()\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "\n",
        "            if isinstance(column_name, str):\n",
        "                if column_name in self.numerical:\n",
        "                    var = self.df[column_name]\n",
        "                    var.plot.density(title=f\"Plot de densidad para la variable {column_name}\")\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/{column_name}.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "                else:\n",
        "                    var = self.df[column_name]\n",
        "                    histogram(var,N= N_adj)\n",
        "                    plt.gcf()\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/{column_name}.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "                \n",
        "\n",
        "            elif isinstance(column_name,list):\n",
        "                present_num = [i for i in self.numerical if i in column_name]\n",
        "                present_cat = [i for i in self.categorical if i in column_name]\n",
        "\n",
        "                if present_num and len(present_num) > 1:\n",
        "                    correlation_plot(present_num)\n",
        "                    plt.gcf()\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/corr.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                if present_cat and len(present_cat) > 1:\n",
        "                    cramer_v_plot(present_cat)\n",
        "                    plt.gcf()\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/cramer_v.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                for col in column_name:\n",
        "                    if col in self.numerical.columns:\n",
        "                        var = self.df[col]\n",
        "                        var.plot.density(title = f\"Plot de densidad para la variable {col}\")\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "                    if col in self.categorical.columns:\n",
        "                        histogram(self.df[col],N= N_adj)\n",
        "                        plt.gcf()\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Exception Error: {e}')\n",
        "\n",
        "    def clean_data(self,non_atomic=None, column_name=None, everything=True, nan_treatment=\"delete\"):\n",
        "        try:\n",
        "            if os.path.exists(self.CLEAN_directory_name):\n",
        "                shutil.rmtree(self.CLEAN_directory_name)\n",
        "            os.mkdir(self.CLEAN_directory_name)\n",
        "            print(f\"Directory '{self.CLEAN_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        \n",
        "\n",
        "        file_path = f\"{self.CLEAN_directory_name}/data.csv\"\n",
        "\n",
        "        if everything:\n",
        "            to_clean = self.df\n",
        "        else:\n",
        "            to_clean = self.df[column_name]\n",
        "\n",
        "        def atomize_column(X, column_name_not_atomic):\n",
        "            split_names = column_name_not_atomic.split('-')\n",
        "            n_splits = len(split_names)\n",
        "            new_df = pd.DataFrame(columns=split_names)\n",
        "\n",
        "            def process_split(x):\n",
        "                split = re.findall(r'\\d+\\.\\d+', x)\n",
        "                if len(split) < n_splits:\n",
        "                    split += [np.nan] * (n_splits - len(split))\n",
        "                return {split_names[i]: split[i] for i in range(n_splits)}\n",
        "\n",
        "            non_atomic_data = X[column_name_not_atomic].apply(process_split)\n",
        "            new_df = pd.DataFrame(list(non_atomic_data))\n",
        "            return new_df\n",
        "\n",
        "        def clean_data_func(X, non_atomic=None, column_name=None, nan_treatment=\"delete\"):\n",
        "            df = X.copy()\n",
        "\n",
        "            if non_atomic != None:\n",
        "                for i in non_atomic:\n",
        "                    to_add = atomize_column(df, i)\n",
        "                    df = pd.concat([df, to_add], axis=1)\n",
        "                    df = df.drop(i, axis=1)\n",
        "            df = df.drop_duplicates()\n",
        "\n",
        "            if nan_treatment == \"delete\":\n",
        "                df = df.dropna()\n",
        "            elif nan_treatment == \"mean\":\n",
        "                df = df.apply(pd.to_numeric, errors='coerce')\n",
        "                df = df.fillna(df.mean())\n",
        "            elif nan_treatment == \"zeros\":\n",
        "                df = df.fillna(0)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid nan_treatment option!\")\n",
        "            return df\n",
        "\n",
        "        clean_data_transformer = FunctionTransformer(clean_data_func,kw_args={'non_atomic': non_atomic,\\\n",
        "                    'column_name': column_name,'nan_treatment': nan_treatment})\n",
        "\n",
        "        # Apply the FunctionTransformer to your DataFrame\n",
        "        df_limpio = clean_data_transformer.transform(to_clean)\n",
        "\n",
        "        # Save the cleaned DataFrame to a CSV file\n",
        "        df_limpio.to_csv(f\"{self.CLEAN_directory_name}/data.csv\", index=False)\n",
        "        print(\"Percentage of data retained:\",np.round((df_limpio.shape[0] / to_clean.shape[0])*100,2), '%')\n",
        "\n",
        "\n",
        "    def scale(self, categorical_method=None):\n",
        "        try:\n",
        "            if os.path.exists(self.SCALE_directory_name):\n",
        "                shutil.rmtree(self.SCALE_directory_name)\n",
        "            os.mkdir(self.SCALE_directory_name)\n",
        "            print(f\"Directory '{self.SCALE_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        file_path = f\"{self.SCALE_directory_name}/scaled_features.csv\"\n",
        "        clean_path = f\"{self.CLEAN_directory_name}/data.csv\"\n",
        "        try:\n",
        "            clean_data = pd.read_csv(clean_path)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        num_cols = clean_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "        cat_cols = clean_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "\n",
        "        numerical_transformer = Pipeline(steps=[('log_scaler', FunctionTransformer(np.log10)),('minmax_scaler', MinMaxScaler())])\n",
        "\n",
        "        if categorical_method is not None:\n",
        "            if isinstance(categorical_method, LabelEncoder):\n",
        "                categorical_transformer = FunctionTransformer(\n",
        "                    lambda x: x.apply(lambda col: categorical_method.fit_transform(col.astype(str)).astype(int)))\n",
        "            else:\n",
        "                categorical_transformer = Pipeline(steps=[('categorical_encoder', categorical_method)])\n",
        "            preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, cat_cols)])\n",
        "        else:\n",
        "            preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols)])\n",
        "\n",
        "        pipe = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "        transformed_data = pipe.fit_transform(clean_data)\n",
        "\n",
        "        if categorical_method is not None:\n",
        "            if isinstance(categorical_method, LabelEncoder):\n",
        "                cat_transformed_cols = cat_cols\n",
        "            else:\n",
        "                cat_transformed_cols = preprocessor.named_transformers_['cat'].named_steps['categorical_encoder'].get_feature_names_out(cat_cols)\n",
        "            if isinstance(categorical_method, OneHotEncoder) and categorical_method.sparse:\n",
        "                transformed_data = transformed_data.toarray()\n",
        "            transformed_cols = num_cols.tolist() + cat_transformed_cols.tolist()\n",
        "        else:\n",
        "            transformed_cols = num_cols.tolist()\n",
        "\n",
        "        transformed_df = pd.DataFrame(transformed_data, columns=transformed_cols)\n",
        "        transformed_df.to_csv(file_path, index=False)\n",
        "\n",
        "    def make_clusters(self, clustering_algorithm, n_clusters = None, columns_name=None):\n",
        "\n",
        "        \"\"\"Create clusters using K-Means and visualize the results.\n",
        "\n",
        "        This function creates clusters from the input data using K-Means clustering.\n",
        "        It can also automatically determine the optimal number of clusters using\n",
        "        the elbow method. The resulting clusters are visualized in a scatter plot\n",
        "        using PCA.\n",
        "\n",
        "        Args:\n",
        "            n_clusters (int, optional): The number of clusters to create. If None, the\n",
        "                function will automatically determine the optimal number of clusters\n",
        "                using the elbow method. Defaults to None.\n",
        "            columns_name (list, optional): List of column names to use for clustering.\n",
        "                If None, all columns from the DataFrame are used. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Note:\n",
        "            If `n_clusters` is not provided, this function will determine the optimal\n",
        "            number of clusters using the elbow method. It is recommended to set\n",
        "            `n_clusters` to None for optimal results.\n",
        "        \"\"\"\n",
        "        def auto_elbow_method(data,n_clusters_range:np.linspace = range(1,15),_n_init = 10, plot_elbow = True):\n",
        "            \"\"\"auto elbow method for kmeans\n",
        "\n",
        "            Args:\n",
        "                data (_type_): data to clusterize\n",
        "                n_clusters_range (np.linspace): range of n_clusters for elbow method\n",
        "                _n_init (int, optional): number of kmeans init. Defaults to 5.\n",
        "                _random_state (int, optional): for replicability. Defaults to 42.\n",
        "                plot (bool, optional): plots elbow method. Defaults to False.\n",
        "\n",
        "            Returns:\n",
        "                _n_clusters_ (int): returns number of clusters for kmeans\n",
        "            \"\"\"\n",
        "            # Range of cluster numbers to try\n",
        "            # Initialize an empty list to store the variance explained by each cluster\n",
        "            inertia = []\n",
        "\n",
        "            # Perform K-Means clustering for different values of k\n",
        "            for n_clusters in tqdm(n_clusters_range,desc='testing clusters in elbow method'):\n",
        "                kmeans = KMeans(n_clusters=n_clusters,n_init=_n_init)\n",
        "                kmeans.fit(data)\n",
        "                inertia.append(kmeans.inertia_)\n",
        "\n",
        "            if plot_elbow:\n",
        "                # Create the Elbow Method graph\n",
        "                plt.figure()\n",
        "                plt.plot(n_clusters_range, inertia, marker='o')\n",
        "                plt.title('Elbow Method for Optimal K')\n",
        "                plt.xlabel('Number of Clusters (K)')\n",
        "                plt.ylabel('Variance Explained (Inertia)')\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "            variation = [(inertia[i] - inertia[i+1])/ inertia[i] * 100 for i in range(len(inertia)-1)]\n",
        "            n_clusters = n_clusters_range[variation.index(max(variation)) + 1]\n",
        "            print(f\"Optimal n_clusters is {int(n_clusters)}\")\n",
        "            return n_clusters\n",
        "\n",
        "        def reduce_and_plot(data):\n",
        "\n",
        "            pca = PCA()\n",
        "\n",
        "\n",
        "            cluster_pipeline = Pipeline([\n",
        "            ('clustering', clustering_algorithm),\n",
        "            ('pca', pca)\n",
        "            ])\n",
        "            cluster_labels = cluster_pipeline['clustering'].fit_predict(data)\n",
        "            X_pca = cluster_pipeline['pca'].fit_transform(data)\n",
        "            sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cluster_labels, palette='viridis',markers='.',s = 40,legend=False)\n",
        "            plt.title('PCA Scatter Plot with Clusters')\n",
        "            plt.xlabel('PCA Component 1')\n",
        "            plt.ylabel('PCA Component 2')\n",
        "            \n",
        "            plt.show()\n",
        "            return cluster_labels\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(self.CLUSTERS_directory_name):\n",
        "                shutil.rmtree(self.CLUSTERS_directory_name)\n",
        "            os.mkdir(self.CLUSTERS_directory_name)\n",
        "            print(f\"Directory '{self.CLUSTERS_directory_name}' created (overwritten) successfully.\")\n",
        "            scaled_path = f\"{self.SCALE_directory_name}/scaled_features.csv\"\n",
        "\n",
        "            if os.path.exists(scaled_path):\n",
        "                print(f\"The file '{scaled_path}' exists.\")\n",
        "            else:\n",
        "                print(f\"The file '{scaled_path}' does not exist. Calling a method...\")\n",
        "                self.clean_data()\n",
        "                self.scale()\n",
        "            data = pd.read_csv(f\"{self.SCALE_directory_name}/scaled_features.csv\")\n",
        "            \n",
        "            if columns_name is None:\n",
        "                if isinstance(clustering_algorithm,KMeans) and n_clusters is None:\n",
        "                    print(f'n_clusters is set to None. n_clusters will be initialized via the internal auto_elbow_method function.')\n",
        "                    n_clusters =  auto_elbow_method(data,range(1,15))\n",
        "\n",
        "                    print(f\"The optimal number of clusters (k) is: {n_clusters}\")\n",
        "                    clustering_algorithm.set_params(n_clusters = n_clusters)\n",
        "                cluster_labels = reduce_and_plot(data)\n",
        "\n",
        "\n",
        "            elif isinstance(columns_name,list):\n",
        "                data = data[columns_name]\n",
        "                if isinstance(clustering_algorithm,KMeans) and n_clusters is None:\n",
        "                    print(f'n_clusters is set to None. n_clusters will be initialized via the internal auto_elbow_method function.')\n",
        "                    n_clusters =  auto_elbow_method(data,range(1,15))\n",
        "\n",
        "                    print(f\"The optimal number of clusters (k) is: {n_clusters}\")\n",
        "                    clustering_algorithm.set_params(n_clusters = n_clusters)\n",
        "                cluster_labels = reduce_and_plot(data)\n",
        "            \n",
        "            data_with_clusters = pd.DataFrame({'Cluster': cluster_labels})\n",
        "            data_with_clusters = pd.concat([data,data_with_clusters],axis = 1)\n",
        "            data_with_clusters.to_csv(f'{self.CLUSTERS_directory_name}/data_clusters.csv',index=False)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "    def detect_anomalies(self,algoritmo, algoritmo_params=None,tipo=\"limpio\",columns_name = None):\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(self.ANOMALY_directory_name):\n",
        "                shutil.rmtree(self.ANOMALY_directory_name)\n",
        "            os.mkdir(self.ANOMALY_directory_name)\n",
        "            print(f\"Directory '{self.CLUSTERS_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        path_file=f'{self.CLUSTERS_directory_name}/data_anomalies.csv'\n",
        "        \n",
        "        scaled_path = f\"{self.SCALE_directory_name}/scaled_features.csv\"\n",
        "        if os.path.exists(scaled_path):\n",
        "            print(f\"The file '{scaled_path}' exists.\")\n",
        "        else:\n",
        "            print(f\"The file '{scaled_path}' does not exist. Calling a method...\")\n",
        "            self.clean_data()\n",
        "            self.scale()\n",
        "\n",
        "        data = pd.read_csv(scaled_path)\n",
        "        \n",
        "        if isinstance(columns_name,list):\n",
        "            data = data[columns_name]\n",
        "        if algoritmo_params is None:\n",
        "            algoritmo_params = {}\n",
        "\n",
        "        if not (is_classifier(algoritmo) or is_outlier_detector(algoritmo)):\n",
        "            raise ValueError(\"Algoritmo debe ser parte del framework de sklearn\")\n",
        "\n",
        "        model = algoritmo.set_params(**algoritmo_params)\n",
        "        pca = PCA(n_components=2)\n",
        "\n",
        "        pipeline = Pipeline([('pca', pca),\n",
        "                            ('anomaly_detection', model)])\n",
        "\n",
        "        pipeline.fit(data)\n",
        "        predictions = pipeline.predict(data)\n",
        "        data['Anomalias'] = predictions \n",
        "\n",
        "        print(pipeline.steps[0])\n",
        "        X_pca = pipeline['pca'].fit_transform(data)\n",
        "        sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=data['Anomalias'], palette='viridis',markers='.')\n",
        "        plt.title('PCA Scatter Plot with Clusters')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Este metodo ejecuta todos los metodos de la clase exceptuando clearGarbage y este mismo metodo\n",
        "    Cabe decir que se ejecutar√°n todos los metodos en su configuraci√≥n por defecto\n",
        "    \"\"\"\n",
        "    def profile(self):\n",
        "        \"\"\"\n",
        "        Executes all callable methods of the class, excluding specific methods.\n",
        "\n",
        "        Calls all callable methods of the class except those listed in 'no_incluir' list.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        metodos = [method for method in dir(self) if callable(getattr(self, method)) and not method.startswith(\"__\")]\n",
        "        no_incluir = [\"profile\", \"clearGarbage\"]\n",
        "        for nombre in metodos:\n",
        "            if nombre not in no_incluir:\n",
        "                metodo = getattr(self, nombre)\n",
        "                metodo()\n",
        "\n",
        "    def clearGarbage(self):\n",
        "        \"\"\"\n",
        "        Deletes all contents within the 'EDA_fecha' folder but leaves the folder itself.\n",
        "\n",
        "        Removes all files and subfolders within the 'EDA_fecha' directory while keeping the directory itself.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        for raiz, carpetas, archivos in os.walk(self.EDA_directory_name, topdown=False):\n",
        "            for folder in carpetas:\n",
        "                folder_path = os.path.join(raiz, folder)\n",
        "                shutil.rmtree(folder_path)\n",
        "            for file in archivos:\n",
        "                file_path = os.path.join(raiz, file)\n",
        "                os.remove(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "nNEkogGrUPjn",
        "outputId": "33a9d9ff-0dcd-4151-ac8f-8b1e2600bd55"
      },
      "outputs": [],
      "source": [
        "\n",
        "test = pd.read_parquet('olimpiadas.parquet')\n",
        "display(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20qn4zttUPjn"
      },
      "outputs": [],
      "source": [
        "#esta columna extra es para poder probar la funcion de matriz de correlacion.\n",
        "test['num_for_test'] = test['Year'].apply(lambda x: x*np.random.random()).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkZ6HMhRfYBe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtxB4EWAy6xY"
      },
      "source": [
        "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
        "\n",
        "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
        "\n",
        "1. Introducci√≥n\n",
        "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
        "\n",
        "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
        "\n",
        "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
        "\n",
        "- Describir la tarea asociada al dataset.\n",
        "- Describir brevemente los datos de entrada que les provee el problema.\n",
        "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
        "\n",
        "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
        "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
        "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
        "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
        "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
        "        - ¬øExisten datos duplicados en el conjunto?\n",
        "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
        "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
        "3. Creaci√≥n de Clusters y Anomal√≠as\n",
        "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
        "    \n",
        "4. An√°lisis de Resultados\n",
        "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
        "    \n",
        "5. Conclusi√≥n\n",
        "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLruh20Zy6xY"
      },
      "source": [
        "## 1. Introducci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLMT7qPXGxyW"
      },
      "source": [
        "Una de las principales tareas en el mundo del Data Science es lograr proveer insights sobre la data que se est√° estudiando. Comunmente, la data puede ser de gran volumen, con muchos datapoints y m√∫ltiples atributos. Para procesar esta data es necesario construir funcionalidades que procesen la data y para esto el uso de programaci√≥n orientada a objetos es la mejor opci√≥n.\n",
        "\n",
        "En esta ocasi√≥n se ha solicitado llevar a cabo un an√°lisis sobre el dataset `'olimpiadas.parquet'`, de manera de obtener los mencionados insights. El dataset provisto cuenta con informaci√≥n acerca de los resultados de los juegos ol√≠mpicos a trav√©s del tiempo. Se mencionan los participantes, su nacionalidad, a√±o de participaci√≥n, temporada (juegos de invierno o verano), ciudad, disciplina, evento, si se ha ganado medalla y detalles acerca de la edad, el peso y la altura del participante.\n",
        "\n",
        "Si bien se podr√≠a construir un perfilador (profiler) que estudiara autom√°ticamente el dataset en espec√≠fico, se ha solicitado que el perfilador sea agn√≥stico al dataset de entrada, siendo  la caracterizaci√≥n autom√°tica, cosa que esta soluci√≥n sea aplicable a cualquier dataset.\n",
        "\n",
        "Para esto, es posible construir una clase de Python que lleve a cabo este procesamiento, siendo agn√≥stica a la data que se le ingresa y provea insights automatizados sobre la data. Esta es la hip√≥tesis que se busca confirmar durante este proyecto. Gracias a la programaci√≥n orientada a objetos, es posible crear un programa que reciba data como entrada, y a trav√©s de los atributos definidos en esta clase, y los m√©todos programados para estudiar la data, generar insights.\n",
        "\n",
        "En este proyecto se construy√≥ la clase `Profiler()`, la cual recibe un dataframe y crea un perfil del dataset a trav√©s de sus m√©todos.\n",
        "\n",
        "La clase Profiler() contiene los sigueintes m√©todos:\n",
        "1. `summarize()`: caracteriza las variables del dataset entregado al objeto profiler.\n",
        "2. `plot_vars`: grafica las variables del dataset. Puede graficar todas las columnas del dataset o bien recibir una lista de nombres de las columnas del dataset. El m√©todo graficar√° correlaci√≥n para las variables num√©ricas, cramer V para las variables categ√≥ricas, densidad para las variables num√©ricas y gr√°ficos de frecuencia para las variables categ√≥ricas. El n√∫mero de columnas admitidas en los gr√°ficos de frecuencia es ajustable a trav√©s del par√°metro `N_adj`.\n",
        "3. `clean_data()`: limpia los datos. Es parte de las opciones de preprocesamiento de la clase.\n",
        "4. `scale()`: escala la data, es parte de las opciones de preprocesamiento de la clase.\n",
        "5. `make_clusters()`: lanza una instancia de Kmeans() de sklearn. El n√∫mero de clusters puede ser entregado al algoritmo a trav√©s del par√°metro `n_clusters`. De lo contrario, se asignar√° un n√∫mero √≥ptimo de clusters a trav√©s del m√©todo del codo.\n",
        "6. `detect_anomalies()` detecta anomal√≠as basado enun algoritmo de detecci√≥n de anomal√≠as provisto por el usuario.\n",
        "7. `profile()`: ejecuta todos los m√©todos anteriores.\n",
        "8. `clearGarbage()` elimina todos los archivos generados por profiler.\n",
        "\n",
        "Adem√°s, cuenta con los sigueintes atributos:\n",
        "1. `self.df`: almacena el dataset a perfilar.\n",
        "2. `self.current_date`: almacena la fecha actual para construir el directorio donde se almacenar√°n los resultados del perfilador.\n",
        "3. `self.EDA_directory_name`: directorio donde se almacenan los resultados del m√©todo EDA.\n",
        "4. `self.PLOT__directory`: directorio donde se almacenan los gr√°ficos generados.\n",
        "5. `self.CLEAN_directory_name`: directorio donde se almacenan los datos limpiados.\n",
        "6. `self.CLUSTERS_directory_name`: directorio donde se almacenan los resultados de la generaci√≥n de clusters.\n",
        "7. `self.SCALE_directory_name`: directorio donde se almacenan los resultados del escalamiento.\n",
        "8. `self.categorical`: almacena un dataframe que contiene solo las variables categ√≥ricas del dataframe original.\n",
        "9. `self.numerical`: almacena un dataframe que contiene solo las variables num√©ricas del dataframe original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV5SWiqkGxyX"
      },
      "source": [
        "## 2.An√°lisis EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnLC0hMOGxyX"
      },
      "source": [
        "Como mencionado anteriormente, el dataset entregado en esta ocasi√≥n cuenta con diversa informaci√≥n acerca de distintos deportistas en la historia de los juegos ol√≠mpicos. La clase `Profiler()` programada permitir√° estudiar estos datos de manera r√°pida, ya que generar√° informaci√≥n √∫til respecto al dataset con simplemente llamar a los m√©todos que tiene. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En primer lugar,se carga el dataset y se visualizan algunos de sus datos. Debido a las limitaciones computacionales a la hora de correr este dataset, se ha fijado el n√∫mero de elementos  a procesar en 100000, sin embargo la clase programada puede recibir un dataset de cualquier tama√±o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn9hUNDDGxyX",
        "outputId": "a806b196-10e5-486e-f269-fe8bbf9b03b5"
      },
      "outputs": [],
      "source": [
        "test = pd.read_parquet('olimpiadas.parquet')[:100000]\n",
        "display(test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es posible ver las columnas Name, Sex, Team, NOC, Games, etc. Corresponden a distintos atributos asociados al atleta, al cual se le refiere por nombre en la columna \"Name\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUuDaNyWGxyY"
      },
      "source": [
        "Para inicializar el perfilador se llama a la clase `Profiler()` y se le entrega el dataset de inter√©s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxHL0w6KGxyY",
        "outputId": "fe245553-2ed3-43cd-95ba-1d01b15a79c2"
      },
      "outputs": [],
      "source": [
        "test_class = Profiler(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un mensaje printeado en la consola indica que se ha creado un directorio. En este se almacenar√° la data generada por el perfilador. Si el directorio ya exist√≠a, ser√° sobreescrito para evitar confusiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuQwF5dIGxyY"
      },
      "source": [
        "La clase `Profiler()` contiene como atributo al dataframe entregado a la inicializaci√≥n. Esto significa que al llamar *self.df* se pueda acceder al dataframe entregado. Aqu√≠ se pueden aprovechar las funciones sobre dataframes que provee pandas, como por ejemplo el m√©todo `.describe()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "B-WwJoXHGxyY",
        "outputId": "0831b1c5-1175-47f9-c324-4676d6fe95eb"
      },
      "outputs": [],
      "source": [
        "test_class.df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La primera herramienta interna del objeto `Profile()` disponible para estudiar la data es el m√©todo `.summarize()`, el cual genera un detallado resumen del comportamiento global de la data, y lo exporta al archivo \"summarize.txt\". Este queda autom√°ticamente disponible en el directorio mencionado justo anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El m√©todo printea a la consola el archivo \"summary.txt\" por defecto, sin embargo con la opci√≥n `print_summarize = False` es posible evitar este comportamiento. Adem√°s, es posible se√±alas a traves de una lista o string que columnas deben ser utilizadas en el summarize. Por defecto, se describen todas las variables del dataset en el archivo \"summary.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHycIPWMGxyZ",
        "outputId": "99c93a97-9132-495e-fe6e-447d0d8a52ab"
      },
      "outputs": [],
      "source": [
        "test_class.summarize()\n",
        "#TODO arreglar docstring\n",
        "#TODO arreglar encoding del escritor del archivo summarize porque entrega weas raras\n",
        "#TODO no se reportan valores duplicados pero lo preguntan en el informe??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNQVxO0PGxyZ"
      },
      "source": [
        "**¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?**\n",
        "> Gracias al reporte generado es posible ver que en este dataset hay variables num√©ricas y categ√≥ricas. Es posible encontrar NaNs en la columna \"Medal\". Son 85684 NaNs, de un total de 100000 datos, osea, al rededor del 85% de los atletas no tiene medalla. \n",
        " \n",
        "\n",
        "**¬øExisten datos duplicados en el conjunto?**\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKdZklfrGxyZ"
      },
      "source": [
        "Tambi√©n se pueden construir visualizaciones de las variables con el m√©todo `plot_vars()`. Este puede recibir una lista con los nombres de las columnas, y de esta manera seleccionar solo algunas columnas para graficar. Sin embargo, por defecto se grafican todas las columnas del dataset. \n",
        "\n",
        "Estos gr√°ficos ser√°n distintos si la columna es num√©rica (se grafica la distribuci√≥n de densidad de esa columna) o categ√≥ricas (se grafican las frecuencias de ocurrencia de cada categor√≠a presentada, pudiendo ajustar el n√∫mero m√°ximo de categor√≠as a trav√©s del par√°metro `N_adj`)\n",
        "\n",
        "M√°s adelante algunos ejemplos de esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una variable num√©rica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhAluRirGxyZ"
      },
      "outputs": [],
      "source": [
        "test_class.plot_vars('Year')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De aqu√≠ se puede concluer que la mayor√≠a de los atletas en el dataset han participado en las olimpiadas alrededor del a√±o 2000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una variable categ√≥rica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_class.plot_vars('Sport', N_adj=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para estudiar el comportamiento inter-variables es posible construir una matriz de correlaci√≥n para las variables num√©ricas, y una matriz de Cramer V para las variables categ√≥ricas. No se entregar ningun par√°metro para que estas se generen. Ocurrir√° de manera autom√°tica si se llama a la funci√≥n plot_vars con una lista de variables. Eso si, tanto para la matriz de correlaci√≥n como la matriz de Cramer V es necesario que hayan al menos dos variables, (al menos dos num√©ricas para correlaci√≥n y al menos dos categ√≥ricas para Cramer. De contrario es imposible calcular estas matrices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Matriz de correlaci√≥n y Cramer V para un subconjunto de las columnas del dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_class.plot_vars(['Year','ID','Sport','Team'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De los gr√°ficos anteriores, generados sin tener m√°s que llamar al m√©todo, es posible extraer un mont√≥n de informaci√≥n. Por ejemplo, que EEUU es el pa√≠s con m√°s deportistas, y por otro lado, el atletismo es el deporte con m√°s atletas. El gr√°fico de 'Year' se analiz√≥ anteriormente, e 'ID'  tiene una distribuci√≥n uniforme, lo cual tiene sentido considerando que en realidad, es el elemento identificador de los deportistas en el dataset.\n",
        "\n",
        "\n",
        "Todos los gr√°ficos generados se almacenan en formato .pdf dentro del directorio correspondiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjiMlTX6GxyZ"
      },
      "source": [
        "**¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?**\n",
        "> Para las variables num√©ricas, se observa una matriz de correlaci√≥n. No se observan correlaciones num√©ricas relevantes para el an√°lisis.\n",
        "\n",
        "\n",
        "**¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?**\n",
        "        \n",
        "**¬øExisten relaciones o patrones visuales entre las variables?**\n",
        ">\n",
        "**¬øExisten anomal√≠as notables o preocupantes en los datos?**\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZg-YVqvGxya"
      },
      "source": [
        "## 3. Clusters y anomal√≠as."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otra funcionalidad implementada en la clase `Profiler()` es la posibilidad de lanzar una clusterizaci√≥n. Sin embargo para esto es necesario limpiar y escalar la data, ya que por ejemplo, no es posible llevar a cabo una clusterizaci√≥n con columnas incompletas, o en columnas no-at√≥micas. \n",
        "\n",
        "Anteriormente se observ√≥ en el summarize que hay columnas con valores NaNs. Estos deben ser tratados para llevar a cabo la clusterizaci√≥n. Por el otro lado, sabemos que hay columnas no at√≥micas, vistas en la primera presentaci√≥n de los datos.\n",
        "\n",
        "Para limpiar la data se puede utilizar el m√©todo `clean_data()`. Este recibe el nombre de la(s) columna(s) no-at√≥micas para atomizarla(s). El par√°metro `everything` indica que se deben llevar a cabo todos los preprocesamientos correspondientes. En la limpieza se llevan a cabo los siguientes procesos:\n",
        "\n",
        "aqui los siguientes procesos. ------\n",
        "\n",
        "`clean_data()` informa en la consola el porcentaje de retenci√≥n de la data y la exporta a un archivo .csv en el directorio asociado a la limpieza.\n",
        "  \n",
        "Si se intenta lanzar `.make_clusters()` sin haber llamado a los m√©todos `clean_data()` y `.scale()`, se llamar√°n a estos dentro de `make_clusters()` con su inicializaci√≥n por defecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mdV0EvqGxya",
        "outputId": "5c768324-97e2-434a-ae70-f27f2b052728"
      },
      "outputs": [],
      "source": [
        "test_class.clean_data(non_atomic=[\"age-height-weight\"],everything=True)\n",
        "#TODO agregar printeo de que se exporto la data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Luego de la limpieza es necesario proceder con un escalamiento para llevar a cabo la clusterizaci√≥n. La funci√≥n `.scale()` escala las variables del archivo *data.csv* generado por `clean_data()` y aplica codificaci√≥n a las variables categ√≥ricas. El m√©todo de codificaci√≥n es explicitado a trav√©s del par√°metro `categorical_method` al llamar a la funci√≥n.\n",
        "\n",
        "La funci√≥n `.scale()` exporta los resultados del procesamiento al directorio correspondiente bajo el nombre *scaled_features.csv*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhQeAuaOGxya",
        "outputId": "5a42fa1d-4ab2-409e-839f-70127a4c6be2"
      },
      "outputs": [],
      "source": [
        "test_class.scale(categorical_method=LabelEncoder())\n",
        "#test_class.scale(categorical_method=OneHotEncoder())\n",
        "#TODO agregar mensajes de q se ha exportado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez llevado a cabo el procesamiento necesario para hacer una clusterizaci√≥n, se puede llamar al m√©todo `.make_clusters()`. Este m√©todo recibe el algoritmo de clusterizaci√≥n provisto por el usuario para llevar a cabo una clusterizaci√≥n sobre los datos escalados. La data se importa directamente del archivo creado con el m√©todo `.scale()` llamado *scaled_features.csv*. \n",
        "\n",
        "Es posible seleccionar solo algunas columnas de la data a trav√©s de la lista `columns_name`, como ocurre a continuaci√≥n. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk05k-JRGxyb"
      },
      "source": [
        "## 4.Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1SJ0HW35Gxya",
        "outputId": "41cc3296-7dba-4fc2-9fc1-d2e352244218"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans()\n",
        "#test_class.make_clusters(kmeans)\n",
        "test_class.make_clusters(kmeans,columns_name=['Games','Season','City','age','weight'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es posible observar que para este dataset en particular, la elecci√≥n de las variables a clusterizar es extremadamente importante para encontrar clusters efectivos. Eligiendo *'Games','Season','City','age','weight'* como variables a clusterizar, es posible encontrar grupos. Sin embargo esto no ocurre al utilizar todas las columnas del dataset, por ejemplo. Adem√°s, se observa que el m√©todo de elecci√≥n de clusters autom√°tico funciona bien.\n",
        "\n",
        "A continuaci√≥n se muestran los resultados de utilizar otros m√©todos de clusterizaci√≥n, y con distintas combinaciones de columnas. Se observa un clustering inefectivo.\n",
        "\n",
        "`.make_clusters()` exporta el dataset con el n√∫mero de cluster asignado al directorio correspondiente bajo el nombre *data_clusters.csv*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "1NmQU0emGxyb",
        "outputId": "819ffc68-ac86-4c52-afc6-40fb1f7f9a1a"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN(eps=0.3, min_samples=2)\n",
        "test_class.make_clusters(dbscan,columns_name=['Season','City'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "Y09epWSFGxyb",
        "outputId": "7bcba36c-0b33-4b25-f69d-eec25be0c7f1"
      },
      "outputs": [],
      "source": [
        "agg= AgglomerativeClustering()\n",
        "test_class.make_clusters(agg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detecci√≥n de anomal√≠as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La detecci√≥n de anomal√≠as es importante porque, como dice su nombre, permite detectar anomal√≠as en la data. Al no ser detectadas estas podr√≠an introducir problemas. Por ejemplo, podr√≠an transmitir un bias si se construye un modelo, podr√≠an movilizar err√≥neamente variables estad√≠sticas como el promedio, la desviaci√≥n est√°ndar, etc. \n",
        "\n",
        "El algoritmo IsolationForest() es una buena opci√≥n como algoritmo de detecci√≥n de anomal√≠as. Se comporta adecuadamente ante grandes vol√∫menes de data e identifica hasta los casos menos comunes de ocurrencia dentro de un dataset (en otras palabras, detecta a los outliers m√°s dificiles de detectar). Se considera un algoritmo robusto ante la mayor√≠a de las distribuciones de datos, siendo agn√≥stico a estas variaciones. \n",
        "\n",
        "En el m√©todo `.detect_anomalies()` se ha implementado un pipeline de procesamiento de detecci√≥n de anomal√≠as. La funci√≥n recibe un algoritmo de detecci√≥n de anomal√≠as y utiliza la data de *scaled_features.csv* para encontrar outliers. De no haberse creado los archivos correspondientes (*scaled_features.csv*) llama a los m√©todos necesarios para crear este archivo. \n",
        "\n",
        "Finalmente, luego de la detecci√≥n de anomal√≠as se llama a una reducci√≥n de dimensionalida PCA la cual permite visualizar las anomal√≠as detectadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq9BYZ3DGxyb"
      },
      "outputs": [],
      "source": [
        "test_class.detect_anomalies(IsolationForest(), columns_name=['Games','Season','City','age','weight'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qIT4lAOGxyb"
      },
      "source": [
        "## 5. Conclusi√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, dado que puede que el perfilador genere mucha data, es posible llamar al m√©todo `.clearGarbage()` para borrar toda la data generada en el directorio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bxbwDMrKtuj"
      },
      "outputs": [],
      "source": [
        "test_class.clearGarbage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzEqo8mQGxyc"
      },
      "source": [
        "En este proyecto se ha visto la efectividad de construir una clase perfiladora agn√≥stica al dataset, puesto que ahora ser√° posible ejecutar m√∫ltiples EDAs sobre cualquier dataset de manera r√°pida y eficaz."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vk05k-JRGxyb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
