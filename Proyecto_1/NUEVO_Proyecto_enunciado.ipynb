{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vTs1ax9y6xN"
      },
      "source": [
        "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE5KA3OEy6xQ"
      },
      "source": [
        "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
        "\n",
        "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesor: Ignacio Meza, Gabriel Iturra\n",
        "- Auxiliar: Sebasti√°n Tinoco\n",
        "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
        "\n",
        "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
        "\n",
        "### Equipo:\n",
        "\n",
        "- Simon Repolt\n",
        "- Magdalena de la Fuente\n",
        "\n",
        "\n",
        "### Link de repositorio de GitHub: `https://github.com/HexaPulsar/LabMDS`\n",
        "\n",
        "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLU9ydRby6xQ"
      },
      "source": [
        "----\n",
        "\n",
        "## Reglas\n",
        "\n",
        "- **Grupos de 2 personas.**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Estrictamente prohibida la copia.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNpMtVMZy6xR"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
        "</div>\n",
        "\n",
        "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
        "\n",
        "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
        "- Caracterizaci√≥n autom√°tica de los datos\n",
        "- La soluci√≥n debe ser compatible con cualquier dataset\n",
        "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMutfXyfy6xR"
      },
      "source": [
        "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
        "\n",
        "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
        "\n",
        "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
        "\n",
        "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Reportar el tipo de variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
        "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
        "    - Si la variables es num√©rica:\n",
        "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
        "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
        "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
        "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
        "\n",
        "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/plots`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Para las variables num√©ricas:\n",
        "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
        "        - Grafique la correlaci√≥n entre las variables\n",
        "    - Para las variables categ√≥ricas:\n",
        "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
        "        - Grafique el coeficiente V de Cramer entre las variables\n",
        "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
        "    \n",
        "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clean_data`\n",
        "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
        "    - Drop de valores duplicados\n",
        "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
        "        - Drop de valores nulos\n",
        "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
        "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
        "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
        "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
        "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
        "    - Deber√≠an usar `FunctionTransformer`.\n",
        "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
        "\n",
        "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/scale`\n",
        "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
        "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
        "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
        "        - Asuma que no existen datos ordinales en su dataset\n",
        "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
        "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
        "\n",
        "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "    - Crear la carpeta `EDA_fecha/clusters`\n",
        "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
        "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
        "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering.\n",
        "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
        "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
        "\n",
        "    - Crear la carpeta `EDA_fecha/anomalies`\n",
        "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
        "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
        "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a.\n",
        "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
        "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
        "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
        "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
        "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
        "\n",
        "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
        "\n",
        "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
        "\n",
        "Algunas consideraciones generales:\n",
        "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset.\n",
        "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
        "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
        "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh8A54h0y6xT"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import shutil\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler,FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWG_VNX4y6xV"
      },
      "outputs": [],
      "source": [
        "#pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrDhWdwky6xW"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Profiler():\n",
        "#TODO documentar los metodos\n",
        "    \"\"\" la clase profiler recibira un dataframe de pandas en el constructor\n",
        "    La funci√≥n summarize recibe una LISTA de strings de los nombres de las variables (nombre de las columnas del pandas) a las cuales se desea verle su\n",
        "    informacion respectiva\n",
        "    \"\"\"\n",
        "    def __init__(self,df:pd.DataFrame) -> None:\n",
        "        self.df = df\n",
        "        self.current_date = datetime.datetime.now().strftime(f\"%d-%m-%Y\")\n",
        "\n",
        "        self.EDA_directory_name = f\"EDA_{self.current_date}\"\n",
        "        self.PLOT_directory_name = f\"{self.EDA_directory_name}/plots\"\n",
        "        self.CLEAN_directory_name = f\"{self.EDA_directory_name}/clean_data\"\n",
        "        self.CLUSTERS_directory_name = f\"{self.EDA_directory_name}/clusters\"\n",
        "        self.SCALE_directory_name = f\"{self.EDA_directory_name}/scaled\"\n",
        "        self.categorical = self.df.select_dtypes(include=['object'])  # or exclude=['number'] for older versions\n",
        "        self.numerical = self.df.select_dtypes(include=['number'])  # or include=['number'] for older versions\n",
        "\n",
        "        # Define the directory name\n",
        "        try:\n",
        "            if os.path.exists(self.EDA_directory_name):\n",
        "                shutil.rmtree(self.EDA_directory_name)\n",
        "            os.mkdir(self.EDA_directory_name)\n",
        "            print(f\"Directory '{self.EDA_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "    def summarize(self,column_name:str or list=None):\n",
        "        \"\"\"summarize: recibe  un string con el nombre de una columna o una lista que contenga el nombre de las columnas a realizar resumen\n",
        "        Si se desea realizar un resumen al dataframe completo dejar la variable everything como True y column_name como None.\n",
        "        En caso de que se desee realizar un resumen por variable(s) seleccionada(s), dejar everything en falso e indicar la o las variables\n",
        "        en el formato ya dicho.\n",
        "\n",
        "        Esta funci√≥n provee un txt guardado en la carpeta EDA_fecha con informacion del dataset o las variables, segun corresponda\n",
        "\n",
        "        Raises:\n",
        "            Exception: _description_\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        def detect_outliers(column):\n",
        "            Q1 = column.quantile(0.25)\n",
        "            Q3 = column.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = (column < lower_bound) | (column > upper_bound)\n",
        "            return outliers.sum()\n",
        "\n",
        "        file_path = f\"{self.EDA_directory_name}/summary.txt\"\n",
        "        def report_txt(vars):\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    #print(\"Writing types...\")\n",
        "                    tipos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.dtypes))\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    f.write(\"Los tipos de las variables son: \"+\"\\n\")\n",
        "                    f.write(tipos)\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    #print(\"Writing NaNs...\")\n",
        "                    nan_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.isna().sum().to_list()))\n",
        "                    f.write(\"Cantidad de NaNs por variable: \"+\"\\n\")\n",
        "                    f.write(nan_values)\n",
        "                    f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    #print(\"Writing unique values...\")\n",
        "                    unique_values = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", vars, vars.nunique().to_list()))\n",
        "                    f.write(\"Cantidad de valores √∫nicos por variable: \"+\"\\n\")\n",
        "                    f.write(unique_values)\n",
        "                    if self.numerical.shape[0] > 1: #si no hay numericas no hacer nada de esto\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        nceros = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, (self.numerical == 0).sum().to_list()))\n",
        "                        f.write(\"Cantidad de ceros en las variables numericas: \"+\"\\n\")\n",
        "                        f.write(nceros)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        promedios =  '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.mean().to_list()))\n",
        "                        f.write(\"Promedios de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(promedios)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        negativos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, (self.numerical < 0).sum().to_list()))\n",
        "                        f.write(\"Cantidad de negativos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(negativos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        maximos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.max().to_list()))\n",
        "                        f.write(\"Maximos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(maximos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        minimos = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.min().to_list()))\n",
        "                        f.write(\"M√≠nimos de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(minimos)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        outliers = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, self.numerical.apply(detect_outliers)))\n",
        "                        f.write(\"Outliers de las variables numericas: \"+\"\\n\")\n",
        "                        f.write(outliers)\n",
        "                        f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                        values_perc = [0.25, 0.5, 0.75, 1.0]\n",
        "                        percentiles = self.numerical.quantile(values_perc).values\n",
        "                        for idx,perc in enumerate(values_perc):\n",
        "                            f.write(\"Percentil \" + str(perc*100) + \" de las variables numericas: \"+\"\\n\")\n",
        "                            perc_val = '\\n'.join(map(lambda x, y: f\"{x}: {y}\", self.numerical, percentiles[idx]))\n",
        "                            f.write(perc_val)\n",
        "                            f.write(\"\\n\"+60*\"-\"+\"\\n\")\n",
        "                    f.close()\n",
        "                    print(f\"Summary.txt has been successfully exported too {self.EDA_directory_name}\")\n",
        "                    return\n",
        "        try:\n",
        "            if column_name is None:\n",
        "              report_txt(self.df)\n",
        "\n",
        "            elif isinstance(column_name, str):\n",
        "                var = self.df[[column_name]]\n",
        "                report_txt(var)\n",
        "\n",
        "            elif isinstance(column_name,list):\n",
        "                if not column_name:\n",
        "                    raise ValueError(\"The input columns list is empty\")\n",
        "                vars =  self.df[column_name]\n",
        "                report_txt(vars)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Exception Error: {e}')\n",
        "\n",
        "\n",
        "    def plot_vars(self,column_name= None,N_adj = 10):\n",
        "        \"\"\"\n",
        "        Plots various visualizations for the specified column(s) in the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            column_name (str or list): The name of the column or a list of column names to visualize.\n",
        "            N_adj (int, optional): The number of top categories to consider in histograms. Defaults to 10.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the input columns list is empty.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.PLOT_directory_name):\n",
        "                shutil.rmtree(self.PLOT_directory_name)\n",
        "            os.mkdir(self.PLOT_directory_name)\n",
        "            print(f\"Directory '{self.PLOT_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        def correlation_plot(variables:list):\n",
        "\n",
        "            \"\"\"\n",
        "            Plots a correlation matrix heatmap for a list of numerical variables.\n",
        "\n",
        "            Args:\n",
        "                variables (list): A list of column names representing numerical variables.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "\n",
        "               #TODO revisar que los elementos de la lista sean todos str try/except\n",
        "            correlation_matrix = np.corrcoef(self.numerical[variables], rowvar=False)\n",
        "            plt.figure()  # Set the figure size\n",
        "\n",
        "            # Create a heatmap using Seaborn\n",
        "            sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt='.2f', xticklabels=variables, yticklabels=variables)\n",
        "            plt.title('Correlation Matrix')\n",
        "\n",
        "\n",
        "        def histogram(var: pd.Series, N: int):\n",
        "            \"\"\"\n",
        "            Plots a histogram for a categorical variable.\n",
        "\n",
        "            Args:\n",
        "                var (pd.Series): A pandas Series representing a categorical variable.\n",
        "                N (int): The number of top categories to display in the histogram.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "            category_counts = var.value_counts()\n",
        "            top_N_categories = category_counts.head(N)\n",
        "            plt.bar(top_N_categories.index, top_N_categories.values)\n",
        "            plt.xlabel('Category')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.title(f'Top {N} Categories Histogram for variable {var.name}')\n",
        "            #return fig\n",
        "\n",
        "        def cramers_v(confusion_matrix):\n",
        "            \"\"\"\n",
        "            Calculates Cramer's V statistic for two categorical variables.\n",
        "\n",
        "            Args:\n",
        "                confusion_matrix (pd.DataFrame): The contingency table of two categorical variables.\n",
        "\n",
        "            Returns:\n",
        "                float: The Cramer's V statistic.\n",
        "\n",
        "            Notes:\n",
        "                Cramer's V is a measure of association for categorical variables.\n",
        "\n",
        "            \"\"\"\n",
        "            chi2 = chi2_contingency(confusion_matrix)[0]\n",
        "            n = confusion_matrix.sum().sum()\n",
        "            phi2 = chi2 / n\n",
        "            r, k = confusion_matrix.shape\n",
        "            phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
        "            rcorr = r - ((r - 1)**2) / (n - 1)\n",
        "            kcorr = k - ((k - 1)**2) / (n - 1)\n",
        "            return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "        def cramer_v_plot(variables):\n",
        "            \"\"\"\n",
        "            Plots a heatmap of Cramer's V values for all pairs of categorical variables.\n",
        "\n",
        "            Args:\n",
        "                variables (list): A list of column names representing categorical variables.\n",
        "\n",
        "            Returns:\n",
        "                None\n",
        "            \"\"\"\n",
        "            # Extract categorical columns from the DataFrame\n",
        "            categorical_cols = self.categorical[variables]\n",
        "\n",
        "            # Calculate Cramer's V for all pairs of categorical columns\n",
        "            results = pd.DataFrame(index=categorical_cols.columns, columns=categorical_cols.columns, dtype=float)\n",
        "\n",
        "            for col1 in categorical_cols.columns:\n",
        "                for col2 in categorical_cols.columns:\n",
        "                    confusion_matrix = pd.crosstab(categorical_cols[col1], categorical_cols[col2])\n",
        "                    results.loc[col1, col2] = cramers_v(confusion_matrix)\n",
        "\n",
        "            # Create a heatmap to visualize Cramer's V\n",
        "            plt.figure()\n",
        "            sns.heatmap(results, annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
        "            plt.title(\"Cramer's V Heatmap\")\n",
        "\n",
        "        try:\n",
        "            if column_name is None:\n",
        "                present_num = self.numerical\n",
        "                present_cat = self.categorical\n",
        "\n",
        "                if len(present_num) > 1:\n",
        "                    correlation_plot(present_num.columns)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/corr.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                if len(present_cat) > 1:\n",
        "                    cramer_v_plot(present_cat.columns)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/cramer_v.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                for col in self.df.columns:\n",
        "                    if col in self.numerical.columns:\n",
        "                        var = self.df[col]\n",
        "                        var.plot.density(title = f\"Plot de densidad para la variable {col}\")\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "                    if col in self.categorical.columns:\n",
        "                    #TODO agregar N top categories en histplot\n",
        "                        histogram(self.df[col],N= N_adj)\n",
        "                        plt.gcf()  # Get the current figure\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "\n",
        "            if isinstance(column_name, str):\n",
        "                var = self.df[column_name]\n",
        "                var.plot.density(title=f\"Plot de densidad para la variable {column_name}\")\n",
        "                plt.savefig(f\"{self.PLOT_directory_name}/{column_name}.pdf\", format = 'pdf')\n",
        "                plt.show()\n",
        "\n",
        "            elif isinstance(column_name,list):\n",
        "                present_num = [i for i in self.numerical if i in column_name]\n",
        "                present_cat = [i for i in self.categorical if i in column_name]\n",
        "\n",
        "                if present_num and len(present_num) > 1:\n",
        "                    correlation_plot(present_num)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/corr.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                if present_cat and len(present_cat) > 1:\n",
        "                    cramer_v_plot(present_cat)\n",
        "                    plt.gcf()  # Get the current figure\n",
        "                    plt.savefig(f\"{self.PLOT_directory_name}/cramer_v.pdf\", format = 'pdf')\n",
        "                    plt.show()\n",
        "\n",
        "                for col in column_name:\n",
        "                    if col in self.numerical.columns:\n",
        "                        var = self.df[col]\n",
        "                        var.plot.density(title = f\"Plot de densidad para la variable {col}\")\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "                    if col in self.categorical.columns:\n",
        "                    #TODO agregar N top categories en histplot\n",
        "                        histogram(self.df[col],N= N_adj)\n",
        "                        plt.gcf()  # Get the current figure\n",
        "                        plt.savefig(f\"{self.PLOT_directory_name}/{col}.pdf\", format = 'pdf')\n",
        "                        plt.show()\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'Exception Error: {e}')\n",
        "\n",
        "\n",
        "    def clean_data(self,non_atomic:list=None,column_name: str or list= None, everything=True, nan_treatment = \"delete\"):\n",
        "        \"\"\"\n",
        "        Cleans the data based on the specified parameters and generates a cleaned .csv file.\n",
        "\n",
        "        Args:\n",
        "            non_atomic (list or None): A list of non-atomic column names in the dataset.\n",
        "            If there is only one non-atomic column, it should still be provided as a list. If there are no non-atomic columns, set this parameter to None.\n",
        "            If you want to clean the atomic column along with specific other variables, include the atomic column name in column_name.\n",
        "            column_name (list or str): A list of column names or a single column name to be cleaned.\n",
        "            everything (bool): Set to True to clean the entire dataset. If True, non_atomic should be provided as needed, and column_name should be None.\n",
        "            nan_treatment (str): Options for handling NaN values, which include: \"delete\" (remove rows with NaNs), \"mean\" (replace NaNs with the column's mean for numerical columns), and \"zeros\" (replace NaNs with zeros).\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Notes:\n",
        "            This function generates a .csv file containing the cleaned variables in the 'EDA_fecha/clean/' folder.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.CLEAN_directory_name):\n",
        "                shutil.rmtree(self.CLEAN_directory_name)\n",
        "            os.mkdir(self.CLEAN_directory_name)\n",
        "            print(f\"Directory '{self.CLEAN_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        file_path=f\"{self.CLEAN_directory_name}/data.csv\"\n",
        "        if everything:\n",
        "            to_clean = self.df\n",
        "            columnas = to_clean.columns.to_list()\n",
        "        else:\n",
        "            to_clean =  self.df[column_name]\n",
        "            columnas = column_name\n",
        "        def clean_data_func(df, non_atomic=None, column_name=None, everything=True, nan_treatment=\"delete\"):\n",
        "            if everything == True and column_name is not None:\n",
        "                raise Exception(\"No puedes entregar variables especificas con everything en True!\")\n",
        "\n",
        "            if non_atomic is not None and non_atomic in columnas\n",
        "                for name_col in non_atomic:\n",
        "                    non_atomic_data = df[name_col].str.split(r'[^0-9]+', expand=True)\n",
        "                    n_new_atomic = non_atomic_data.shape[1]\n",
        "                    new_names = [f\"{name_col}_atomic_{i+1}\" for i in range(n_new_atomic)]\n",
        "                    df[new_names] = non_atomic_data\n",
        "                    df[new_names] = df[new_names].apply(pd.to_numeric)\n",
        "                    df.drop(name_col, axis=1, inplace=True)\n",
        "\n",
        "            not_dupe = df.drop_duplicates()\n",
        "\n",
        "            if nan_treatment == \"delete\":\n",
        "                not_na = not_dupe.dropna()\n",
        "            elif nan_treatment == \"mean\":\n",
        "                not_na = not_dupe.fillna(not_dupe.mean())\n",
        "            elif nan_treatment == \"zeros\":\n",
        "                not_na = not_dupe.fillna(0)\n",
        "            else:\n",
        "                raise Exception(\"Tratamiento no disponible!\")\n",
        "\n",
        "            return not_na\n",
        "\n",
        "        clean_data_transformer = FunctionTransformer(clean_data_func, \\\n",
        "            kw_args={'non_atomic': non_atomic, 'column_name': column_name, 'everything': everything, 'nan_treatment': nan_treatment})\n",
        "\n",
        "        # Apply the FunctionTransformer to your DataFrame\n",
        "        df_limpio = clean_data_transformer.transform(to_clean)\n",
        "\n",
        "        # Save the cleaned DataFrame to a CSV file\n",
        "        df_limpio.to_csv(\"EDA_fecha/clean_data/data.csv\", index=False)\n",
        "        print(\"porcentaje de data retenida: \",df_limpio.shape[0]/to_clean.shape[0])\n",
        "        not_na.to_csv(file_path)\n",
        "        self.df = df_limpio\n",
        "\n",
        "\n",
        "    \"\"\"Scale: Este metodo escala los datos numericos primero aplicando logaritmo y despues un escalamiento MinMax\n",
        "\n",
        "    En caso de que se desee realizar escalamiento a los datos categ√≥ricos, ingresar un escalamiento del framework de sklearn\n",
        "    en la variable categorical_method. Si no, dejar como None\n",
        "\n",
        "    Este metodo guarda los datos escalados a partir de los datos limpios guardados en el metodo anterior. Los datos escalados\n",
        "    son guardados en la carpeta EDA_fecha/scale\n",
        "    \"\"\"\n",
        "    def scale(self,categorical_method=None):\n",
        "        \"\"\"\n",
        "        Scales the numerical data by first applying logarithm transformation and then using Min-Max scaling.\n",
        "\n",
        "        Args:\n",
        "        categorical_method: Scikit-learn scaling method to be applied to categorical data. If scaling is not needed for categorical data, leave this parameter as None.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "\n",
        "        Notes:\n",
        "        This method scales the data based on the previously cleaned data and saves the scaled data in the 'EDA_fecha/scale' folder.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.SCALE_directory_name):\n",
        "                shutil.rmtree(self.SCALE_directory_name)\n",
        "            os.mkdir(self.SCALE_directory_name)\n",
        "            print(f\"Directory '{self.SCALE_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        file_path = f\"{self.SCALE_directory_name}/scaled_features.csv\"\n",
        "        clean_path = f\"{self.CLEAN_directory_name}/data.csv\"\n",
        "        clean_data = pd.read_csv(clean_path)\n",
        "\n",
        "        categoricas = clean_data.select_dtypes(include=['object']).columns\n",
        "        numericas = clean_data.select_dtypes(include=['number']).columns\n",
        "\n",
        "        numericas_transf = Pipeline(steps=[('log', FunctionTransformer(func=np.log10)),('minmax', MinMaxScaler())])\n",
        "        if categorical_method != None:\n",
        "            categoricas_transf = Pipeline(steps=[('categoricas', eval(categorical_method)())])\n",
        "            preprocessor = ColumnTransformer(transformers=[('var_cat', categoricas_transf, categoricas),('var_num', numericas_transf, numericas)])\n",
        "        else:\n",
        "            preprocessor = ColumnTransformer(transformers=[('var_num', numericas_transf, numericas)])\n",
        "        data_procesada = preprocessor.fit_transform(clean_data)\n",
        "        self.df = pd.DataFrame(data_procesada)\n",
        "        self.df.to_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "    def make_clusters(self, n_clusters = None, columns_name=None):\n",
        "        \"\"\"Create clusters using K-Means and visualize the results.\n",
        "\n",
        "        This function creates clusters from the input data using K-Means clustering.\n",
        "        It can also automatically determine the optimal number of clusters using\n",
        "        the elbow method. The resulting clusters are visualized in a scatter plot\n",
        "        using PCA.\n",
        "\n",
        "        Args:\n",
        "            n_clusters (int, optional): The number of clusters to create. If None, the\n",
        "                function will automatically determine the optimal number of clusters\n",
        "                using the elbow method. Defaults to None.\n",
        "            columns_name (list, optional): List of column names to use for clustering.\n",
        "                If None, all columns from the DataFrame are used. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Note:\n",
        "            If `n_clusters` is not provided, this function will determine the optimal\n",
        "            number of clusters using the elbow method. It is recommended to set\n",
        "            `n_clusters` to None for optimal results.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.CLUSTERS_directory_name):\n",
        "                shutil.rmtree(self.CLUSTERS_directory_name)\n",
        "            os.mkdir(self.CLUSTERS_directory_name)\n",
        "            print(f\"Directory '{self.CLUSTERS_directory_name}' created (overwritten) successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        if columns_name == None:\n",
        "            copy_df = self.df\n",
        "        else:\n",
        "            copy_df = self.df[columns_name]\n",
        "            #TODO add preprocessing just in case\n",
        "\n",
        "        def auto_elbow_method(data,n_clusters_range:np.linspace = np.linspace(2,10, 8,dtype = int),_n_init = 5, plot_elbow = True,**kwargs):\n",
        "            \"\"\"auto elbow method for kmeans\n",
        "\n",
        "            Args:\n",
        "                data (_type_): data to clusterize\n",
        "                n_clusters_range (np.linspace): range of n_clusters for elbow method\n",
        "                _n_init (int, optional): number of kmeans init. Defaults to 5.\n",
        "                _random_state (int, optional): for replicability. Defaults to 42.\n",
        "                plot (bool, optional): plots elbow method. Defaults to False.\n",
        "\n",
        "            Returns:\n",
        "                _n_clusters_ (int): returns number of clusters for kmeans\n",
        "            \"\"\"\n",
        "            # Range of cluster numbers to try\n",
        "            # Initialize an empty list to store the variance explained by each cluster\n",
        "            inertia = []\n",
        "\n",
        "            # Perform K-Means clustering for different values of k\n",
        "            for n_clusters in tqdm(n_clusters_range,desc='testing clusters in elbow method'):\n",
        "                kmeans = KMeans(n_clusters=n_clusters,n_init=_n_init)\n",
        "                kmeans.fit(data)\n",
        "                inertia.append(kmeans.inertia_)\n",
        "\n",
        "            if plot_elbow:\n",
        "                # Create the Elbow Method graph\n",
        "                plt.figure()\n",
        "                plt.plot(n_clusters_range, inertia, marker='o')\n",
        "                plt.title('Elbow Method for Optimal K')\n",
        "                plt.xlabel('Number of Clusters (K)')\n",
        "                plt.ylabel('Variance Explained (Inertia)')\n",
        "                plt.grid(True)\n",
        "                plt.show()\n",
        "            variation = [(inertia[i] - inertia[i+1])/ inertia[i] * 100 for i in range(len(inertia)-1)]\n",
        "            n_clusters = n_clusters_range[variation.index(max(variation)) + 1]\n",
        "            print(f\"El n√∫mero √≥ptimo de clusters es {int(n_clusters)}\")\n",
        "            return n_clusters\n",
        "\n",
        "        pca = PCA(n_components=2)\n",
        "        if n_clusters is None:\n",
        "            n_clusters =  auto_elbow_method(self.df)\n",
        "        # Create a KMeans clustering instance\n",
        "        kmeans = KMeans(n_clusters=n_clusters)\n",
        "\n",
        "        # Create a pipeline with PCA and KMeans\n",
        "        cluster_pipeline = Pipeline([\n",
        "        ('pca', pca),\n",
        "        ('kmeans', kmeans)\n",
        "        ])\n",
        "\n",
        "        # Fit the pipeline to the data\n",
        "        cluster_pipeline.fit(self.df)\n",
        "\n",
        "        # Transform the data using PCA\n",
        "        X_pca = cluster_pipeline['pca'].transform(self.df)\n",
        "\n",
        "        # Get the cluster labels from KMeans\n",
        "        cluster_labels = cluster_pipeline['kmeans'].labels_\n",
        "\n",
        "        # Create a scatter plot of the PCA results\n",
        "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
        "        plt.title('PCA Scatter Plot with Clusters')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.show()\n",
        "\n",
        "    def detect_anomalies():\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Este metodo ejecuta todos los metodos de la clase exceptuando clearGarbage y este mismo metodo\n",
        "    Cabe decir que se ejecutar√°n todos los metodos en su configuraci√≥n por defecto\n",
        "    \"\"\"\n",
        "    def profile(self):\n",
        "        \"\"\"\n",
        "        Executes all callable methods of the class, excluding specific methods.\n",
        "\n",
        "        Calls all callable methods of the class except those listed in 'no_incluir' list.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        metodos = [method for method in dir(self) if callable(getattr(self, method)) and not method.startswith(\"__\")]\n",
        "        no_incluir = [\"profile\", \"clearGarbage\"]\n",
        "        for nombre in metodos:\n",
        "            if nombre not in no_incluir:\n",
        "                metodo = getattr(self, nombre)\n",
        "                metodo()\n",
        "\n",
        "    \"Este metodo borra todos los contenidos de la carpeta EDA_fecha, pero no esta carpeta.\"\n",
        "    def clearGarbage(self):\n",
        "        \"\"\"\n",
        "        Deletes all contents within the 'EDA_fecha' folder but leaves the folder itself.\n",
        "\n",
        "        Removes all files and subfolders within the 'EDA_fecha' directory while keeping the directory itself.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        for raiz, carpetas, archivos in os.walk(self.EDA_directory_name, topdown=False):\n",
        "            for folder in carpetas:\n",
        "                folder_path = os.path.join(raiz, folder)\n",
        "                shutil.rmtree(folder_path)\n",
        "            for file in archivos:\n",
        "                file_path = os.path.join(raiz, file)\n",
        "                os.remove(file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "nNEkogGrUPjn",
        "outputId": "16a72c25-a48f-4c30-c841-c6acde0914cf"
      },
      "outputs": [],
      "source": [
        "test = pd.read_parquet('olimpiadas.parquet')\n",
        "display(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20qn4zttUPjn"
      },
      "outputs": [],
      "source": [
        "#esta columna extra es para poder probar la funcion de matriz de correlacion.\n",
        "test['num_for_test'] = test['Year'].apply(lambda x: x*np.random.random()).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r5MOJkQy6xX"
      },
      "source": [
        "## testing class initializer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYSM2hJjUPjn"
      },
      "source": [
        "Inicializar la clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4OFTxICUPjn",
        "outputId": "a9956f74-48f8-4c9a-dfe9-048323647b58"
      },
      "outputs": [],
      "source": [
        "test_class = Profiler(test[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb2nphrsUPjo"
      },
      "source": [
        "### Testing summarize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84x0j8iKuahV"
      },
      "source": [
        "None provided\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH4wFo-tuahV",
        "outputId": "45f6ae36-3d59-4d19-90ba-f3199a89da83"
      },
      "outputs": [],
      "source": [
        "test_class.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXgBkKHqUPjo"
      },
      "source": [
        "#### Empty list of columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGcyR-cTuahV",
        "outputId": "0e91b80e-675b-4d8a-d209-a8f04a367402"
      },
      "outputs": [],
      "source": [
        "test_class.summarize([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI4P_nYCUPjo"
      },
      "source": [
        "#### Single variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeBNn42-uahW",
        "outputId": "571469bf-e49b-4343-be8f-8b91e353c384"
      },
      "outputs": [],
      "source": [
        "test_class.summarize(['Year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFHmv9CvUPjp",
        "outputId": "856c2af2-17b7-4d4e-e1bb-7d700c89bbdf"
      },
      "outputs": [],
      "source": [
        "test_class.summarize('Year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDThp6pVuahX",
        "outputId": "68b695a5-e322-498c-9e96-7eb37a70bcfc"
      },
      "outputs": [],
      "source": [
        "test_class.summarize('Sport')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q1wP615UPjo"
      },
      "source": [
        "#### Two numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQG1nOfJuahX",
        "outputId": "c3ba6479-3825-407a-b3d3-2985e7738437"
      },
      "outputs": [],
      "source": [
        "test_class.summarize(['Year','num_for_test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVn0hcBkUPjp"
      },
      "source": [
        "#### Two categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChyKrorJuahY",
        "outputId": "888c6ad6-814f-4ddf-c23a-af0ac2fccaf8"
      },
      "outputs": [],
      "source": [
        "test_class.summarize(['Sport','NOC'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ADzbAhUPjp"
      },
      "source": [
        "#### Categorical and Numerical mix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_El1tsbfy6xX",
        "outputId": "9a0a21aa-01be-45a3-f427-5bcdac959d22"
      },
      "outputs": [],
      "source": [
        "test_class.summarize(['Year','Sport'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsMgjCtSUPjp"
      },
      "source": [
        "### Testing plot_vars()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4YQUjaWUPjp"
      },
      "source": [
        "#### Empty list of columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4KG1giXUPjq",
        "outputId": "c8ec3abf-7a4b-4ac7-c6e5-31fbef4b3713"
      },
      "outputs": [],
      "source": [
        "test_class.plot_vars([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgXMTxCeuahZ",
        "outputId": "2c10fe0e-210f-4f5c-b579-9b8f83f181cf"
      },
      "outputs": [],
      "source": [
        "test_class.plot_vars()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oEI7IyrUPjq"
      },
      "source": [
        "#### Single variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf9g64DbUPjq",
        "outputId": "a3b4e081-67f8-444a-affe-2575227dd247"
      },
      "outputs": [],
      "source": [
        "test_class.plot_vars('Year')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKH1Ao2SUPjq"
      },
      "source": [
        "#### Two numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bmvcYEEUPjq",
        "outputId": "4ee2dfc0-2835-47d1-9ad2-d277a86eb392"
      },
      "outputs": [],
      "source": [
        "test_class.plot_vars(['Year',\"num_for_test\"],N_adj=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oma-tAYMUPjr"
      },
      "source": [
        "#### Two categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pomrHQl4UPjr",
        "outputId": "b3d67c2d-06e1-4a64-ad2c-fbbe625824c5"
      },
      "outputs": [],
      "source": [
        "test_class.plot_vars([\"Sport\",\"NOC\"],N_adj=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BVbJ_25UPjr"
      },
      "source": [
        "#### Categorical and Numerical mix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2xJ7lscUPjr",
        "outputId": "578a6fb8-82cb-4e49-cdfb-f0eacca409d3"
      },
      "outputs": [],
      "source": [
        "test_class.plot_vars(['Year',\"num_for_test\",\"Sport\",\"Team\",\"NOC\"],N_adj=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0u4k9piUPjr"
      },
      "source": [
        "### Testing  clean_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56vp-Rl9UPjs"
      },
      "source": [
        "#### Empty list of columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-w9ewrWUPjs"
      },
      "source": [
        "#### Single variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Mn8wrTUPjs"
      },
      "source": [
        "#### Two numerical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4yyzplSUPjx"
      },
      "source": [
        "#### Two categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku9h1VVUUPjx"
      },
      "source": [
        "#### Categorical and Numerical mix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcngEh76UPjy"
      },
      "source": [
        "### Testing scale()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dAeZ6DNUPjy"
      },
      "source": [
        "#### Empty list of columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_QQtMEzUPjy"
      },
      "source": [
        "#### Single variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3cr4tnYUPjy"
      },
      "source": [
        "#### Two numerical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JECH-v7xUPjy"
      },
      "source": [
        "#### Two categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfB2MO0jUPjy"
      },
      "source": [
        "#### Categorical and Numerical mix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLMHHBpHUPjz"
      },
      "source": [
        "### Testing make_clusters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oFLfeMgDTEr",
        "outputId": "f947be69-a226-4f17-99ad-ea728e47174a"
      },
      "outputs": [],
      "source": [
        "test_class.clean_data()\n",
        "#display(test_class.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc3yr6qvDTEs",
        "outputId": "f4ea1964-97e6-4d2a-8836-f453a421e712"
      },
      "outputs": [],
      "source": [
        "test_class.scale()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODyBma0yUPjz"
      },
      "source": [
        "#### Kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4fmonZCUPj0",
        "outputId": "16f2e73f-91d9-4cda-9d0f-f7c78cecdd31"
      },
      "outputs": [],
      "source": [
        "test_class.make_clusters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEjfSQGfuahn",
        "outputId": "46acee29-8434-4fbc-fa01-c9bfa18aab09"
      },
      "outputs": [],
      "source": [
        "test_class.make_clusters(n_clusters = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV78M5RJUPj1"
      },
      "source": [
        "### Testing detect_anomalies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z71stm5UPj1"
      },
      "source": [
        "#### Empty list of columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HecYTWfCUPj1"
      },
      "source": [
        "#### Single variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EBXtKuqUPj1"
      },
      "source": [
        "#### Two numerical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMuQbLHzUPj2"
      },
      "source": [
        "#### Two categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjoGXrD2UPj2"
      },
      "source": [
        "#### Categorical and Numerical mix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eylda2MyUPj2"
      },
      "source": [
        "### Testing profile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ9dYnaGuahp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAshTP-WUPj3"
      },
      "source": [
        "### Testing clearGarbage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfPkd0Bguahp"
      },
      "outputs": [],
      "source": [
        "test_class.clearGarbage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtxB4EWAy6xY"
      },
      "source": [
        "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
        "\n",
        "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
        "\n",
        "1. Introducci√≥n\n",
        "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
        "\n",
        "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
        "\n",
        "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
        "\n",
        "- Describir la tarea asociada al dataset.\n",
        "- Describir brevemente los datos de entrada que les provee el problema.\n",
        "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
        "\n",
        "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
        "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
        "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
        "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
        "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
        "        - ¬øExisten datos duplicados en el conjunto?\n",
        "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
        "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
        "3. Creaci√≥n de Clusters y Anomal√≠as\n",
        "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
        "    \n",
        "4. An√°lisis de Resultados\n",
        "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
        "5. Conclusi√≥n\n",
        "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLruh20Zy6xY"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
